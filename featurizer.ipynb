{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rdkit\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from rdkit import Chem, AllChem\n",
    "from deepchem.feat.smiles_tokenizer import BasicSmilesTokenizer\n",
    "from rdkit.Chem import Draw, AllChem, Descriptors, rdDepictor, rdDistGeom, MACCSkeys, rdMolDescriptors\n",
    "from rdkit.Chem import rdDepictor\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric import utils as pyg_utils\n",
    "from torch_geometric.data import InMemoryDataset, download_url, extract_gz, Data, DataLoader, Batch\n",
    "\n",
    "# # 작업을 위한 별도의 함수 불러오기\n",
    "# from utils.download_preprocess import CustomMoleculeNet, atom_features, EDGE_FEATURES\n",
    "\n",
    "# 시각화를 위한 라이브러리\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(rdkit.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CHARSMISET = {\"(\": 1, \".\": 2, \"0\": 3, \"2\": 4, \"4\": 5, \"6\": 6, \"8\": 7, \"@\": 8,\n",
    "                \"B\": 9, \"D\": 10, \"F\": 11, \"H\": 12, \"L\": 13, \"N\": 14, \"P\": 15, \"R\": 16,\n",
    "                \"T\": 17, \"V\": 18, \"Z\": 19, \"\\\\\": 20, \"b\": 21, \"d\": 22, \"f\": 23, \"h\": 24,\n",
    "                \"l\": 25, \"n\": 26, \"r\": 27, \"t\": 28, \"#\": 29, \"%\": 30, \")\": 31, \"+\": 32,\n",
    "                \"-\": 33, \"/\": 34, \"1\": 35, \"3\": 36, \"5\": 37, \"7\": 38, \"9\": 39, \"=\": 40,\n",
    "                \"A\": 41, \"C\": 42, \"E\": 43, \"G\": 44, \"I\": 45, \"K\": 46, \"M\": 47, \"O\": 48,\n",
    "                \"S\": 49, \"U\": 50, \"W\": 51, \"Y\": 52, \"[\": 53, \"]\": 54, \"a\": 55, \"c\": 56,\n",
    "                \"e\": 57, \"g\": 58, \"i\": 59, \"m\": 60, \"o\": 61, \"s\": 62, \"u\": 63, \"y\": 64,\n",
    "                'p': 65, '~': 66, '>': 67, '<': 68} # add p, ~, >, <\n",
    "\n",
    "CHARISOSMILEN = 68\n",
    "\n",
    "CHARPROTSET = {\"A\": 1, \"C\": 2, \"B\": 3, \"E\": 4, \"D\": 5, \"G\": 6,\n",
    "               \"F\": 7, \"I\": 8, \"H\": 9, \"K\": 10, \"M\": 11, \"L\": 12,\n",
    "               \"O\": 13, \"N\": 14, \"Q\": 15, \"P\": 16, \"S\": 17, \"R\": 18,\n",
    "               \"U\": 19, \"T\": 20, \"W\": 21, \"V\": 22, \"Y\": 23, \"X\": 24, \"Z\": 25}\n",
    "\n",
    "CHARPROTLEN = 25\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "########## Function\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "def integer_label_encoding(sequence, tp, max_length=100):\n",
    "    \"\"\"\n",
    "    Integer encoding for string sequence.\n",
    "    Args:\n",
    "        sequence (str): Drug or Protein string sequence.\n",
    "        max_length: Maximum encoding length of input string.\n",
    "    \"\"\"\n",
    "    if tp == 'drug':\n",
    "        charset = CHARSMISET\n",
    "    elif tp == 'protein':\n",
    "        charset = CHARPROTSET\n",
    "\n",
    "    encoding = np.zeros(max_length)\n",
    "    for idx, letter in enumerate(sequence[:max_length]):\n",
    "        try:\n",
    "            if tp == 'protein':\n",
    "                letter = letter.upper()\n",
    "            letter = str(letter)\n",
    "            encoding[idx] = charset[letter]\n",
    "        except KeyError:\n",
    "            print(\n",
    "                f\"character {letter} does not exists in sequence category encoding, skip and treat as padding.\"\n",
    "            )\n",
    "    return Data(x=torch.from_numpy(encoding).to(torch.long).unsqueeze(dim=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit import RDLogger\n",
    "from pathlib import Path\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import add_self_loops\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "RDLogger.DisableLog('rdApp.*')  \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def smiles_to_coord(smiles):\n",
    "    try:\n",
    "        # SMILES → Mol\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            print(f\"[Warning] MolFromSmiles failed for: {smiles}\")\n",
    "            return None\n",
    "        \n",
    "        mol = Chem.AddHs(mol)\n",
    "\n",
    "        # 3D 좌표 생성\n",
    "        status = AllChem.EmbedMolecule(mol, randomSeed=42)\n",
    "        if status == -1:\n",
    "            AllChem.Compute2DCoords(mol)\n",
    "            print(f\"[Warning] EmbedMolecule failed for: {smiles}\")        \n",
    "        else:\n",
    "            AllChem.UFFOptimizeMolecule(mol)\n",
    "\n",
    "        # conformer 가져오기\n",
    "        if mol.GetNumConformers() == 0:\n",
    "            print(f\"[Warning] No conformer generated for: {smiles}\")\n",
    "            return None\n",
    "        conf = mol.GetConformer()\n",
    "\n",
    "        # 원자 번호와 좌표 추출\n",
    "        z = []\n",
    "        pos = []\n",
    "        for atom in mol.GetAtoms():\n",
    "            z.append(atom.GetAtomicNum())\n",
    "            p = conf.GetAtomPosition(atom.GetIdx())\n",
    "            pos.append([p.x, p.y, p.z])\n",
    "        \n",
    "        z = torch.tensor(z, dtype=torch.long)\n",
    "        pos = torch.tensor(pos, dtype=torch.float)\n",
    "        return Data(z=z, pos=pos)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[Exception] Failed for {smiles}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... existing code ...\n",
    "\n",
    "# Updated transform_mol function with progress logging\n",
    "from torch_geometric.data import Data\n",
    "from tqdm import tqdm\n",
    "from utils.molecule_feature import *\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "def transform_mol(molecule_smiles, labels, choice):\n",
    "    mols = [Chem.MolFromSmiles(mol) for mol in molecule_smiles if mol]\n",
    "    print(f\"Processing {len(molecule_smiles)} molecules with {choice} transformation...\")\n",
    "    \n",
    "    # string tokenization\n",
    "    if choice == 'string_tokenization': # vocab dictionary, encoded smiles를 출력\n",
    "        print(\"Building vocabulary from SMILES tokens...\")\n",
    "        vocab = []\n",
    "        max_len = 0\n",
    "        tokenizer = BasicSmilesTokenizer()\n",
    "        for smi in tqdm(molecule_smiles, desc=\"Tokenizing SMILES\"):\n",
    "            tokens = tokenizer.tokenize(smi)\n",
    "            max_len = max(max_len, len(tokens))\n",
    "            vocab += tokens\n",
    "            \n",
    "        uniq_vocab = sorted(set(vocab))\n",
    "        smiles_vocab = {v: i for i, v in enumerate(uniq_vocab)}\n",
    "        smiles_vocab['Unk'] = len(smiles_vocab)\n",
    "        print(f\"Vocabulary size: {len(smiles_vocab)}\")\n",
    "        \n",
    "        print(\"Encoding SMILES sequences...\")\n",
    "        encoded_smiles = [[smiles_vocab.get(token, smiles_vocab['Unk']) for token in tokenizer.tokenize(smi)] for smi in tqdm(molecule_smiles, desc=\"Encoding SMILES\")]\n",
    "        smiles_vec = []\n",
    "        for vec, l, smi in tqdm(zip(encoded_smiles, labels, molecule_smiles), desc=\"Creating Data objects\", total=len(molecule_smiles)):\n",
    "            pad_len = max_len - len(vec)\n",
    "            vec = vec + ([0] * pad_len)\n",
    "            smiles_vec.append(Data(x=torch.tensor(vec).view(1, -1), y=torch.tensor([l], dtype=torch.float).view(1, -1), smiles=smi))\n",
    "        print(f\"Completed string tokenization for {len(smiles_vec)} molecules\")\n",
    "        return smiles_vocab, smiles_vec\n",
    "\n",
    "    # integer encoding (CNN)\n",
    "    elif choice == 'integer_encoding':\n",
    "        print(\"Converting SMILES to integer encoding...\")\n",
    "        integer_encoding_data = []\n",
    "        for smi, l in tqdm(zip(molecule_smiles, labels), desc=\"Integer encoding\", total=len(molecule_smiles)):\n",
    "             drug = integer_label_encoding(smi, 'drug')\n",
    "             drug.y = torch.tensor([l], dtype=torch.float).view(1, -1)\n",
    "             integer_encoding_data.append(drug)\n",
    "        print(f\"Completed integer encoding for {len(integer_encoding_data)} molecules\")\n",
    "        return integer_encoding_data\n",
    "\n",
    "    # 2D Graph\n",
    "    elif choice == '2D_graph':\n",
    "        print(\"Converting SMILES to 2D molecular graphs...\")\n",
    "        graph_data = [smiles_to_feature(smi) for smi in tqdm(molecule_smiles, desc=\"Creating 2D graphs\")]\n",
    "\n",
    "        graph_2d = []\n",
    "        for g, l, smi in tqdm(zip(graph_data, labels, molecule_smiles), desc=\"Adding labels to graphs\", total=len(molecule_smiles)):\n",
    "            g.y = torch.tensor([l], dtype=torch.float).view(1, -1)\n",
    "            g.smiles = smi\n",
    "            graph_2d.append(g)\n",
    "        print(f\"Completed 2D graph conversion for {len(graph_2d)} molecules\")\n",
    "        return graph_2d\n",
    "\n",
    "    # 3D Graph\n",
    "    elif choice == '3D_graph':\n",
    "        print(\"Converting SMILES to 3D molecular graphs...\")\n",
    "        graph_3d = []\n",
    "        for smi, l in tqdm(zip(molecule_smiles, labels), desc=\"Creating 3D graphs\", total=len(molecule_smiles)):\n",
    "            graph_data = smiles_to_coord(smi)\n",
    "            if graph_data is None:\n",
    "                print(f\"Failed to create 3D graph for {smi}\")\n",
    "                continue\n",
    "            graph_data.y = torch.tensor([l], dtype=torch.float).view(1, -1)\n",
    "            graph_3d.append(graph_data)\n",
    "        print(f\"Completed 3D graph conversion for {len(graph_3d)} molecules\")\n",
    "        return graph_3d\n",
    "\n",
    "    # ChemBERTa\n",
    "    elif choice == 'chemberta':\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"DeepChem/ChemBERTa-77M-MLM\")\n",
    "        model = AutoModel.from_pretrained(\"DeepChem/ChemBERTa-77M-MLM\")\n",
    "        print(\"Converting SMILES to ChemBERTa embeddings...\")\n",
    "        chemberta_data = []\n",
    "        for smi, l in tqdm(zip(molecule_smiles, labels), desc=\"Creating ChemBERTa embeddings\", total=len(molecule_smiles)):\n",
    "            with torch.no_grad():\n",
    "                inputs = tokenizer(smi, return_tensors='pt', padding=True, truncation=True)\n",
    "                outputs = model(**inputs)\n",
    "                embedding = outputs.last_hidden_state[:, 0, :].squeeze(0)  # (hidden_size,)\n",
    "            data = Data(x=embedding.unsqueeze(0), smiles=smi)\n",
    "            data.y = torch.tensor([l], dtype=torch.float).view(1, -1)\n",
    "            chemberta_data.append(data)\n",
    "        print(f\"Completed ChemBERTa embedding conversion for {len(chemberta_data)} molecules\")\n",
    "        return chemberta_data\n",
    "    \n",
    "    # Fingerprint\n",
    "    elif 'fingerprint' in choice:\n",
    "        print(f\"Generating {choice} fingerprints...\")\n",
    "        if choice == 'rdkit_fingerprint':\n",
    "            fp = [Chem.RDKFingerprint(mol) for mol in tqdm(mols, desc=\"RDKit fingerprints\")]\n",
    "        \n",
    "        elif choice == 'maccs_fingerprint':\n",
    "            fp = [MACCSkeys.GenMACCSKeys(mol) for mol in tqdm(mols, desc=\"MACCS fingerprints\")]\n",
    "        \n",
    "        elif choice == 'morgan_fingerprint':\n",
    "            fp = [AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=1024) for mol in tqdm(mols, desc=\"Morgan fingerprints\")]\n",
    "\n",
    "        print(\"Converting fingerprints to Data objects...\")\n",
    "        fps = [Data(x=torch.tensor(f).view(1, -1), y=torch.tensor([l], dtype=torch.float).view(1, -1), smiles=smi) for f, l, smi in tqdm(zip(fp, labels, molecule_smiles), desc=\"Creating fingerprint Data objects\", total=len(molecule_smiles))]\n",
    "        print(f\"Completed {choice} generation for {len(fps)} molecules\")\n",
    "        return fps\n",
    "\n",
    "    # Descriptors\n",
    "    elif choice == 'descriptors':\n",
    "        print(\"Calculating molecular descriptors...\")\n",
    "        # 모델 학습을 위해서는 스케일링 작업이 별도로 필요하다는 것을 기억하자!\n",
    "        desc = []\n",
    "        for mol, l, smi in tqdm(zip(mols, labels, molecule_smiles), desc=\"Calculating descriptors\", total=len(molecule_smiles)):\n",
    "            x = torch.tensor(list(Descriptors.CalcMolDescriptors(mol).values()), dtype=torch.float).view(1, -1)\n",
    "            y = torch.tensor([l], dtype=torch.float).view(1, -1)\n",
    "            desc.append(Data(x=x, y=y, smiles=smi))\n",
    "        print(f\"Completed descriptor calculation for {len(desc)} molecules\")\n",
    "        return desc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dta_trn = pd.read_csv('dta_dataset/kiba/train.csv')\n",
    "dta_val = pd.read_csv('dta_dataset/kiba/valid.csv')\n",
    "dta_tst = pd.read_csv('dta_dataset/kiba/test.csv')\n",
    "\n",
    "dta_trn['Set'] = 'TRN'\n",
    "dta_val['Set'] = 'VAL'\n",
    "dta_tst['Set'] = 'TST'\n",
    "\n",
    "dta = pd.concat([dta_trn, dta_val, dta_tst]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dta['CNN'] = transform_mol(dta['Drug'], dta['Y'], 'integer_encoding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dta['2D-GNN'] = transform_mol(dta['Drug'], dta['Y'], '2D_graph')\n",
    "dta['FP-Morgan'] = transform_mol(dta['Drug'], dta['Y'], 'morgan_fingerprint') # 1024\n",
    "dta['FP-MACCS'] = transform_mol(dta['Drug'], dta['Y'], 'maccs_fingerprint') # 167"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dta['ChemBERTa'] = transform_mol(dta['Drug'], dta['Y'], 'chemberta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dta['Target_Rep'] = dta['Target'].apply(lambda x: integer_label_encoding(x, 'protein', 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dta['3D-GNN'] = transform_mol(dta['Drug'], dta['Y'], '3D_graph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "fd = Path('dta_dataset/kiba/feature/')\n",
    "fd.mkdir(parents=True, exist_ok=True)\n",
    "for ft in ['CNN']:\n",
    "# for ft in ['3D-GNN', 'ChemBERTa']:\n",
    "# for ft in ['2D-GNN', 'FP-Morgan', 'FP-MACCS', 'CNN']:\n",
    "    nfd = fd / ft\n",
    "    nfd.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    trn_sub = dta[dta['Set'] == 'TRN'][[ft, 'Target_Rep']].reset_index(drop=True).rename(columns={ft: 'Drug_Rep'}).to_dict('records')\n",
    "    val_sub = dta[dta['Set'] == 'VAL'][[ft, 'Target_Rep']].reset_index(drop=True).rename(columns={ft: 'Drug_Rep'}).to_dict('records')\n",
    "    tst_sub = dta[dta['Set'] == 'TST'][[ft, 'Target_Rep']].reset_index(drop=True).rename(columns={ft: 'Drug_Rep'}).to_dict('records')\n",
    "    # print(f'{ft} feature_dim', dta[ft].values[0].x.shape)\n",
    "    \n",
    "    with open(nfd / 'trn.pkl', 'wb') as f:\n",
    "        pickle.dump(trn_sub, f)\n",
    "    with open(nfd / 'val.pkl', 'wb') as f:\n",
    "        pickle.dump(val_sub, f)\n",
    "    with open(nfd / 'tst.pkl', 'wb') as f:\n",
    "        pickle.dump(tst_sub, f)\n",
    "    \n",
    "    print('Saved', nfd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zinc = pd.read_csv('data/zinc/zinc15_250K.csv')\n",
    "zinc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def transform_mol_nolabel(molecule_smiles, choice):\n",
    "#     mols = [Chem.MolFromSmiles(mol) for mol in molecule_smiles if mol]\n",
    "#     print(f\"Processing {len(molecule_smiles)} molecules with {choice} transformation...\")\n",
    "    \n",
    "#      # integer encoding (CNN)\n",
    "#     if choice == 'integer_encoding':\n",
    "#         print(\"Converting SMILES to integer encoding...\")\n",
    "#         integer_encoding_data = {}\n",
    "#         for smi in tqdm(molecule_smiles):\n",
    "#              drug = integer_label_encoding(smi, 'drug')\n",
    "#              integer_encoding_data[smi] = drug\n",
    "#         print(f\"Completed integer encoding for {len(integer_encoding_data)} molecules\")\n",
    "#         return integer_encoding_data\n",
    "\n",
    "#     # 2D Graph\n",
    "#     elif choice == '2D_graph':\n",
    "#         print(\"Converting SMILES to 2D molecular graphs...\")\n",
    "#         graph_data = {smi: drug_to_graph(smi) for smi in tqdm(molecule_smiles, desc=\"Creating 2D graphs\")}\n",
    "#         print(f\"Completed 2D graph conversion for {len(graph_data)} molecules\")\n",
    "#         return graph_data\n",
    "\n",
    "#     # 3D Graph\n",
    "#     elif choice == '3D_graph':\n",
    "#         print(\"Converting SMILES to 3D molecular graphs...\")\n",
    "#         graph_3d = {}\n",
    "#         for smi in tqdm(molecule_smiles):\n",
    "#             graph_data = drug_to_graph(smi)\n",
    "            \n",
    "#             mol = Chem.MolFromSmiles(smi)\n",
    "#             atom_info = [(atom.GetIdx(), atom.GetSymbol()) for atom in mol.GetAtoms()]\n",
    "                     \n",
    "#             mol = AllChem.AddHs(mol, addCoords=True)\n",
    "#             emb_mol = rdDistGeom.EmbedMolecule(mol)\n",
    "#             if emb_mol == -1:\n",
    "#                 rdDepictor.Compute2DCoords(mol)\n",
    "\n",
    "#             conf = mol.GetConformer()\n",
    "#             pos = np.array([conf.GetAtomPosition(idx) for idx, symbol in atom_info])\n",
    "#             graph_data.pos = pos\n",
    "#             graph_3d[smi] = graph_data\n",
    "#         print(f\"Completed 3D graph conversion for {len(graph_3d)} molecules\")\n",
    "#         return graph_3d\n",
    "    \n",
    "#     # Fingerprint\n",
    "#     elif 'fingerprint' in choice:\n",
    "#         print(f\"Generating {choice} fingerprints...\")\n",
    "#         if choice == 'rdkit_fingerprint':\n",
    "#             fp = [Chem.RDKFingerprint(mol) for mol in tqdm(mols, desc=\"RDKit fingerprints\")]\n",
    "        \n",
    "#         elif choice == 'maccs_fingerprint':\n",
    "#             fp = [MACCSkeys.GenMACCSKeys(mol) for mol in tqdm(mols, desc=\"MACCS fingerprints\")]\n",
    "        \n",
    "#         elif choice == 'morgan_fingerprint':\n",
    "#             fp = [AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=1024) for mol in tqdm(mols, desc=\"Morgan fingerprints\")]\n",
    "\n",
    "#         print(\"Converting fingerprints to Data objects...\")\n",
    "#         fps = {smi: Data(x=torch.tensor(f).view(1, -1)) for f, smi in tqdm(zip(fp, molecule_smiles), desc=\"Creating fingerprint Data objects\")}\n",
    "#         print(f\"Completed {choice} generation for {len(fps)} molecules\")\n",
    "#         return fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = '20250628'\n",
    "# for dt in ['bace', 'bbbp', 'tox21', 'toxcast', 'sider', 'clintox', 'hiv']:\n",
    "for dt in ['tox21', 'hiv']:\n",
    "    for ft in ['ChemBERTa']:\n",
    "        cmd = f\"python Train_Property.py --dataset {dt} --feature {ft} --filename {name} --project {dt.upper()}_{ft}_{name}  > ./logs/new/{dt}_{ft}.txt\"\n",
    "        print(cmd)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = '20250710'\n",
    "for dt in ['bace', 'bbbp', 'tox21', 'toxcast', 'sider', 'clintox', 'hiv', 'esol', 'freesolv', 'lipophilicity']:\n",
    "    for ft in ['2D-GNN', '2D-GNN-tuto', 'CNN', 'FP-MACCS', 'FP-Morgan', '3D-GNN', 'ChemBERTa']:\n",
    "        cmd = f\"python Train_Property.py --dataset {dt} --feature {ft} --filename {name} --project {dt.upper()}_{ft}_{name}  > ./logs/new2/{dt}_{ft}.txt\"\n",
    "        print(cmd)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = '20250630'\n",
    "for dt in ['bace', 'bbbp', 'tox21', 'toxcast', 'sider', 'clintox', 'hiv']:\n",
    "    for ft in ['2D-GNN', '2D-GNN-tuto', 'CNN', 'FP-MACCS', 'FP-Morgan', '3D-GNN', 'ChemBERTa']:\n",
    "        cmd = f\"python Train_Property.py --dataset {dt} --feature {ft} --filename {name} --project {dt.upper()}_{ft}_{name}  > ./logs/new2/{dt}_{ft}.txt\"\n",
    "        print(cmd)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.loader import MoleculeDataset ##\n",
    "data_root = \"dataset/\"\n",
    "feature= '2D-GNN'\n",
    "\n",
    "dataset = MoleculeDataset(data_root + 'bace', dataset='bace', feature=feature)\n",
    "dataset = MoleculeDataset(data_root + 'bbbp', dataset='bbbp', feature=feature)\n",
    "dataset = MoleculeDataset(data_root + 'tox21', dataset='tox21', feature=feature)\n",
    "dataset = MoleculeDataset(data_root + 'toxcast', dataset='toxcast', feature=feature)\n",
    "dataset = MoleculeDataset(data_root + 'sider', dataset='sider', feature=feature)\n",
    "dataset = MoleculeDataset(data_root + 'clintox', dataset='clintox', feature=feature)\n",
    "dataset = MoleculeDataset(data_root + 'hiv', dataset='hiv', feature=feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dt_name in ['bace', 'bbbp', 'tox21', 'toxcast', 'sider', 'clintox', 'hiv']:\n",
    "    print(dt_name)\n",
    "    dt1 = MoleculeDataset(data_root + dt_name, dataset=dt_name, feature='2D-GNN')\n",
    "    dt2 = MoleculeDataset(data_root + dt_name, dataset=dt_name, feature='3D-GNN')\n",
    "    check = set(dt1.smiles) & set(dt2.smiles)\n",
    "    print(len(dt1.smiles), len(dt2.smiles), len(check))\n",
    "    if len(check) != len(dt1.smiles):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root:  dataset\\bace\n",
      "feature:  DESC\n",
      "base_smi length:  1513\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from data.loader import MoleculeDataset ##\n",
    "data_root = \"dataset/\"\n",
    "for feature in ['DESC']:\n",
    "    dataset = MoleculeDataset(data_root + 'bace', dataset='bace', feature=feature)\n",
    "    dataset = MoleculeDataset(data_root + 'bbbp', dataset='bbbp', feature=feature)\n",
    "    dataset = MoleculeDataset(data_root + 'tox21', dataset='tox21', feature=feature)\n",
    "    dataset = MoleculeDataset(data_root + 'toxcast', dataset='toxcast', feature=feature)\n",
    "    dataset = MoleculeDataset(data_root + 'sider', dataset='sider', feature=feature)\n",
    "    dataset = MoleculeDataset(data_root + 'clintox', dataset='clintox', feature=feature)\n",
    "    dataset = MoleculeDataset(data_root + 'hiv', dataset='hiv', feature=feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.loader import MoleculeDataset ##\n",
    "data_root = \"dataset/\"\n",
    "for dt_name in ['bace', 'bbbp', 'tox21', 'toxcast', 'sider', 'clintox', 'hiv']:\n",
    "    dataset_base = MoleculeDataset(data_root + dt_name, dataset=dt_name, feature='2D-GNN')\n",
    "    dataset_3d = MoleculeDataset(data_root + dt_name, dataset=dt_name, feature='3D-GNN')\n",
    "\n",
    "    base_smi = '\\n'.join(str(i) for i in dataset_base.smiles)\n",
    "    with open(f'dataset/{dt_name}/processed/base_smi.txt', 'w') as f:\n",
    "        f.write(f'base_smi: {len(dataset_base.smiles)}\\n')\n",
    "        f.write(base_smi)\n",
    "\n",
    "    smi_3d = set(dataset_base.smiles) & set(dataset_3d.smiles)\n",
    "    smi_3d_file = '\\n'.join(str(i) for i in smi_3d)\n",
    "    with open(f'dataset/{dt_name}/processed/3d_smi.txt', 'w') as f:\n",
    "        f.write(f'3d_smi: {len(smi_3d)}\\n')\n",
    "        f.write(smi_3d_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.loader import MoleculeDataset ##\n",
    "data_root = \"dataset/\"\n",
    "# for feature in ['3D-GNN']:\n",
    "for feature in ['2D-GNN', '2D-GNN-tuto', 'CNN', '3D-GNN','FP-MACCS', 'FP-Morgan', 'ChemBERTa']:\n",
    "    print(feature)\n",
    "    dataset = MoleculeDataset(data_root + 'esol', dataset='esol', feature=feature)\n",
    "    dataset = MoleculeDataset(data_root + 'freesolv', dataset='freesolv', feature=feature)\n",
    "    dataset = MoleculeDataset(data_root + 'lipophilicity', dataset='lipophilicity', feature=feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = '20250708'\n",
    "# for dt in ['bace', 'bbbp', 'tox21', 'toxcast', 'sider', 'clintox', 'hiv', 'freesolv', 'esol', 'lipophilicity']:\n",
    "for ft in ['2D-GNN-tuto']:\n",
    "    for dt in ['hiv']:\n",
    "        for tp in ['2L-GCN']:\n",
    "            cmd = f\"python Train_Property.py --dataset {dt} --feature {ft} --filename {name}-{tp} --project {dt.upper()}_{ft}_{name}_{tp}  > ./logs/comp_gcl_tuto_others/{dt}_{ft}_{tp}.txt\"\n",
    "            print(cmd)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = '20250708'\n",
    "for ft in []:\n",
    "    for dt in ['bace', 'bbbp', 'tox21', 'toxcast', 'sider', 'hiv', 'freesolv', 'esol', 'lipophilicity']:\n",
    "        \n",
    "            cmd = f\"python Train_Property.py --dataset {dt} --feature {ft} --filename {name}-{tp} --project {dt.upper()}_{ft}_{name}_{tp}  > ./logs/comp_gcl_tuto_others/{dt}_{ft}_{tp}.txt\"\n",
    "                print(cmd)\n",
    "        if ft == '2D-GNN-copy':\n",
    "            for tp in ['2L-GIN', '5L-GIN', '2L-GCN', '5L-GCN', '2L-GIN-emb', '5L-GIN-emb', '2L-GCN-emb', '5L-GCN-emb', '2L-GIN-emb-fit', '5L-GIN-emb-fit', '2L-GCN-emb-fit', '5L-GCN-emb-fit']:\n",
    "                cmd = f\"python Train_Property.py --dataset {dt} --feature {ft} --filename {name}-{tp} --project {dt.upper()}_{ft}_{name}_{tp}  > ./logs/comp_gcl_tuto_others/{dt}_{ft}_{tp}.txt\"\n",
    "                print(cmd)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = '20250708'\n",
    "# for dt in ['bace', 'bbbp', 'tox21', 'toxcast', 'sider', 'clintox', 'hiv', 'freesolv', 'esol', 'lipophilicity']:\n",
    "for ft in ['2D-GNN-copy']:\n",
    "    for dt in ['bace', 'bbbp', 'tox21', 'toxcast', 'sider', 'hiv', 'freesolv', 'esol', 'lipophilicity']:\n",
    "        if ft in ['2D-GNN-tuto', '2D-GNN-copy2', '2D-GNN-copy3']:\n",
    "            for tp in ['2L-GIN', '5L-GIN', '2L-GCN', '5L-GCN']:\n",
    "                cmd = f\"python Train_Property.py --dataset {dt} --feature {ft} --filename {name}-{tp} --project {dt.upper()}_{ft}_{name}_{tp}  > ./logs/comp_gcl_tuto_others/{dt}_{ft}_{tp}.txt\"\n",
    "                print(cmd)\n",
    "        if ft == '2D-GNN-copy':\n",
    "            for tp in ['2L-GIN', '5L-GIN', '2L-GCN', '5L-GCN', '2L-GIN-emb', '5L-GIN-emb', '2L-GCN-emb', '5L-GCN-emb', '2L-GIN-emb-fit', '5L-GIN-emb-fit', '2L-GCN-emb-fit', '5L-GCN-emb-fit']:\n",
    "                cmd = f\"python Train_Property.py --dataset {dt} --feature {ft} --filename {name}-{tp} --project {dt.upper()}_{ft}_{name}_{tp}  > ./logs/comp_gcl_tuto_others/{dt}_{ft}_{tp}.txt\"\n",
    "                print(cmd)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for dt in ['bace', 'bbbp', 'tox21', 'toxcast', 'sider', 'clintox', 'hiv', 'freesolv', 'esol', 'lipophilicity']:\n",
    "    for ft in ['DESC']:\n",
    "        \n",
    "        cmd = f\"python Train_Property.py --dataset {dt} --feature {ft} --filename {name} --project {dt.upper()}_{ft}_{name}  > ./logs/new4/{dt}_{ft}.txt\"\n",
    "        print(cmd)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.loader import MoleculeDataset ##\n",
    "data_root = \"dataset/\"\n",
    "for dt_name in ['freesolv', 'esol', 'lipophilicity']:\n",
    "    dataset_base = MoleculeDataset(data_root + dt_name, dataset=dt_name, feature='2D-GNN')\n",
    "    dataset_3d = MoleculeDataset(data_root + dt_name, dataset=dt_name, feature='3D-GNN')\n",
    "\n",
    "    base_smi = '\\n'.join(str(i) for i in dataset_base.smiles)\n",
    "    with open(f'dataset/{dt_name}/processed/base_smi.txt', 'w') as f:\n",
    "        f.write(f'base_smi: {len(dataset_base.smiles)}\\n')\n",
    "        f.write(base_smi)\n",
    "\n",
    "    smi_3d = set(dataset_base.smiles) & set(dataset_3d.smiles)\n",
    "    smi_3d_file = '\\n'.join(str(i) for i in smi_3d)\n",
    "    with open(f'dataset/{dt_name}/processed/3d_smi.txt', 'w') as f:\n",
    "        f.write(f'3d_smi: {len(smi_3d)}\\n')\n",
    "        f.write(smi_3d_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "folder = Path('logs/comp_gcl_tuto_others')\n",
    "result = []\n",
    "for file in folder.glob('*.txt'):\n",
    "    fn = file.stem\n",
    "    dn, ftn, mdn = fn.split('_')\n",
    "    \n",
    "    if ftn == '2D-GNN-copy':\n",
    "        mdnn = mdn.split('-')\n",
    "        # print(mdnn)\n",
    "        if mdnn[-1] == 'fit':\n",
    "            pn = 5\n",
    "        elif mdnn[-1] == 'emb':\n",
    "            pn = 4\n",
    "        else:\n",
    "            pn = 3\n",
    "    elif ftn == '2D-GNN-copy2':\n",
    "        pn = 2\n",
    "    elif ftn == '2D-GNN-copy3':\n",
    "        pn = 1\n",
    "    elif ftn == '2D-GNN-tuto':\n",
    "        pn = 0\n",
    "    \n",
    "    print(ftn, mdn, pn)\n",
    "\n",
    "    with open(file, 'r', encoding='utf-16') as f:\n",
    "        lines = f.readlines()\n",
    "        if 'Test Score' in lines[-1]:\n",
    "            result.append({'dataset': dn, 'ft': ftn, 'md': mdn, 'pn': pn, 'score': lines[-1].split(':')[-1].strip()})\n",
    "\n",
    "pd.DataFrame(result).to_excel('summary_results.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = '20250712'\n",
    "for ft in ['FP-MACCS', 'FP-Morgan', 'CNN', 'ChemBERTa', '2D-GNN-tuto', '3D-GNN']:\n",
    "    for dt in ['bace', 'bbbp', 'tox21', 'toxcast', 'sider', 'hiv', 'freesolv', 'esol', 'lipophilicity']:\n",
    "        cmd = f\"python Train_Property_ms.py --dataset {dt} --feature {ft} --filename {name} --project {dt.upper()}_{ft}_{name}  > ./logs/{name}/{dt}_{ft}.txt\"\n",
    "        print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from tqdm import tqdm\n",
    "from rdkit import RDLogger\n",
    "from rdkit.Chem import Descriptors\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "def calc_desc(smi):\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    desc = Descriptors.CalcMolDescriptors(mol)\n",
    "    if desc:\n",
    "        if pd.DataFrame([desc]).isnull().sum().sum() == len(desc):\n",
    "            return None\n",
    "        else:\n",
    "            desc_select = {}\n",
    "            for k, v in desc.items():\n",
    "                if k.startswith('fr'):\n",
    "                    desc_select[k] = v\n",
    "                elif k.startswith('Num') or k.endswith('Count'):\n",
    "                    desc_select[k] = v\n",
    "                elif k in ['qed', 'SPS', 'ExactMolWt', 'MolWt', 'TPSA', 'HeavyAtomMolWt', 'Ipc', 'MolLogP', 'MolMR', 'HallKierAlpha', 'FractionCSP3']:\n",
    "                    desc_select[k] = v\n",
    "            return desc_select\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1513/1513 [00:16<00:00, 92.41it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bace 1513 0 0\n",
      "torch.Size([1, 115])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2050/2050 [00:16<00:00, 126.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bbbp 1972 11 3\n",
      "torch.Size([1, 115])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1483/1483 [00:13<00:00, 110.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clintox 1435 4 25\n",
      "torch.Size([1, 115])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1128/1128 [00:05<00:00, 195.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esol 1121 0 7\n",
      "torch.Size([1, 115])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 642/642 [00:02<00:00, 241.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freesolv 628 0 14\n",
      "torch.Size([1, 115])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41127/41127 [05:56<00:00, 115.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hiv 40759 0 368\n",
      "torch.Size([1, 115])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4200/4200 [00:35<00:00, 118.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lipophilicity 4200 0 0\n",
      "torch.Size([1, 115])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1427/1427 [00:44<00:00, 32.33it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sider 1356 0 71\n",
      "torch.Size([1, 115])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7831/7831 [00:51<00:00, 153.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tox21 7774 0 57\n",
      "torch.Size([1, 115])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8597/8597 [00:57<00:00, 148.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxcast 8536 20 41\n",
      "torch.Size([1, 115])\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from rdkit.Chem import AllChem\n",
    "from data.loader import MoleculeDataset\n",
    "from data.splitters import scaffold_split\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "total = []\n",
    "fds = [fd for fd in Path('dataset').glob('*')]\n",
    "for fd in fds:\n",
    "    data = pd.read_csv([f for f in (fd / 'raw').glob('*.csv')][0])\n",
    "\n",
    "    data_root = \"dataset/\"\n",
    "    dataset = MoleculeDataset(data_root + fd.stem, dataset=fd.stem, feature='CNN')\n",
    "    smiles_list = pd.read_csv(data_root + fd.stem + '/processed/smiles.csv', header=None)[0].tolist()\n",
    "    train_dataset, valid_dataset, test_dataset = scaffold_split(dataset, smiles_list, null_value=0, frac_train=0.8,frac_valid=0.1, frac_test=0.1)\n",
    "    \n",
    "    trn_smiles = sum(train_dataset.smiles, [])\n",
    "    val_smiles = sum(valid_dataset.smiles, [])\n",
    "    tst_smiles = sum(test_dataset.smiles, [])\n",
    "    \n",
    "    if fd.stem == 'bace':\n",
    "        smi_col = 'mol'\n",
    "    else:\n",
    "        smi_col = 'smiles'\n",
    "\n",
    "    box = {'trn': {}, 'val': {}, 'tst': {}}\n",
    "    fail = []\n",
    "    exclude = []\n",
    "    for smi in tqdm(data[smi_col]):\n",
    "        \n",
    "        desc = calc_desc(smi)\n",
    "        if desc:\n",
    "            if fd.stem in ['bbbp', 'toxcast', 'clintox']:\n",
    "                mol = AllChem.MolFromSmiles(smi)\n",
    "                smi = AllChem.MolToSmiles(mol)\n",
    "            \n",
    "            desc['smiles'] = smi\n",
    "            if smi in trn_smiles:\n",
    "                box['trn'][smi] = desc\n",
    "            elif smi in val_smiles:\n",
    "                box['val'][smi] = desc\n",
    "            elif smi in tst_smiles:\n",
    "                box['tst'][smi] = desc\n",
    "            else:\n",
    "                exclude.append(smi)\n",
    "        else:\n",
    "            fail.append(smi)\n",
    "    \n",
    "    trn = pd.DataFrame(box['trn'].values())\n",
    "    val = pd.DataFrame(box['val'].values())\n",
    "    tst = pd.DataFrame(box['tst'].values())\n",
    "    \n",
    "    pre_total = pd.concat([trn, val, tst]).reset_index(drop=True)\n",
    "    pre_total.to_csv(fd / 'processed' / 'desc_pre.csv', index=False)\n",
    "    \n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "    trn[trn.columns[:-1]] = pd.DataFrame(scaler.fit_transform(trn[trn.columns[:-1]]), columns=trn.columns[:-1])\n",
    "    val[val.columns[:-1]] = pd.DataFrame(scaler.transform(val[val.columns[:-1]]), columns=val.columns[:-1])\n",
    "    tst[tst.columns[:-1]] = pd.DataFrame(scaler.transform(tst[tst.columns[:-1]]), columns=tst.columns[:-1])\n",
    "    \n",
    "    total = pd.concat([trn, val, tst]).reset_index(drop=True)\n",
    "\n",
    "    result = {}\n",
    "    for _, row in total.iterrows():\n",
    "        result[row['smiles']] = torch.tensor(list(row.values[:-1])).unsqueeze(0)\n",
    "\n",
    "    with open(fd / 'processed' / 'desc.pkl', 'wb') as f:\n",
    "        pickle.dump(result, f)\n",
    "    \n",
    "    print(fd.stem, len(result), len(fail), len(exclude))\n",
    "    print(result[row['smiles']].shape)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root:  dataset\\bace\n",
      "feature:  DESC\n",
      "base_smi length:  1513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created data:  1513\n",
      "root:  dataset\\bbbp\n",
      "feature:  DESC\n",
      "base_smi length:  1972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created data:  1972\n",
      "root:  dataset\\tox21\n",
      "feature:  DESC\n",
      "base_smi length:  7774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created data:  7774\n",
      "root:  dataset\\toxcast\n",
      "feature:  DESC\n",
      "base_smi length:  8536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created data:  8536\n",
      "root:  dataset\\sider\n",
      "feature:  DESC\n",
      "base_smi length:  1356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created data:  1356\n",
      "root:  dataset\\clintox\n",
      "feature:  DESC\n",
      "base_smi length:  1435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created data:  1435\n",
      "root:  dataset\\hiv\n",
      "feature:  DESC\n",
      "base_smi length:  40759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created data:  40759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Processing...\n",
      "Done!\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root:  dataset\\freesolv\n",
      "feature:  DESC\n",
      "base_smi length:  628\n",
      "Created data:  628\n",
      "root:  dataset\\esol\n",
      "feature:  DESC\n",
      "base_smi length:  1121\n",
      "Created data:  1121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root:  dataset\\lipophilicity\n",
      "feature:  DESC\n",
      "base_smi length:  4200\n",
      "Created data:  4200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from data.loader import MoleculeDataset\n",
    "data_root = \"dataset/\"\n",
    "for feature in ['DESC']:\n",
    "    dataset = MoleculeDataset(data_root + 'bace', dataset='bace', feature=feature)\n",
    "    dataset = MoleculeDataset(data_root + 'bbbp', dataset='bbbp', feature=feature)\n",
    "    dataset = MoleculeDataset(data_root + 'tox21', dataset='tox21', feature=feature)\n",
    "    dataset = MoleculeDataset(data_root + 'toxcast', dataset='toxcast', feature=feature)\n",
    "    dataset = MoleculeDataset(data_root + 'sider', dataset='sider', feature=feature)\n",
    "    dataset = MoleculeDataset(data_root + 'clintox', dataset='clintox', feature=feature)\n",
    "    dataset = MoleculeDataset(data_root + 'hiv', dataset='hiv', feature=feature)\n",
    "    dataset = MoleculeDataset(data_root + 'freesolv', dataset='freesolv', feature=feature)\n",
    "    dataset = MoleculeDataset(data_root + 'esol', dataset='esol', feature=feature)\n",
    "    dataset = MoleculeDataset(data_root + 'lipophilicity', dataset='lipophilicity', feature=feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[4200, 115], smiles=[4200], id=[4200], y=[4200])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from data.loader import MoleculeDataset\n",
    "from data.splitters import scaffold_split\n",
    "\n",
    "data_root = \"dataset/\"\n",
    "dataset = MoleculeDataset(data_root + 'bbbp', dataset='bbbp', feature='DESC')\n",
    "smiles_list = pd.read_csv(data_root + 'bbbp' + '/processed/smiles.csv', header=None)[0].tolist()\n",
    "train_dataset, valid_dataset, test_dataset = scaffold_split(dataset, smiles_list, null_value=0, frac_train=0.8,frac_valid=0.1, frac_test=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1577, 115])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python Train_Property_ms.py --dataset bace --feature DESC --filename 20250716 --project BACE_DESC_20250716  > ./logs/desc/bace_DESC_20250716.txt\n",
      "python Train_Property_ms.py --dataset bbbp --feature DESC --filename 20250716 --project BBBP_DESC_20250716  > ./logs/desc/bbbp_DESC_20250716.txt\n",
      "python Train_Property_ms.py --dataset tox21 --feature DESC --filename 20250716 --project TOX21_DESC_20250716  > ./logs/desc/tox21_DESC_20250716.txt\n",
      "python Train_Property_ms.py --dataset toxcast --feature DESC --filename 20250716 --project TOXCAST_DESC_20250716  > ./logs/desc/toxcast_DESC_20250716.txt\n",
      "python Train_Property_ms.py --dataset sider --feature DESC --filename 20250716 --project SIDER_DESC_20250716  > ./logs/desc/sider_DESC_20250716.txt\n",
      "python Train_Property_ms.py --dataset clintox --feature DESC --filename 20250716 --project CLINTOX_DESC_20250716  > ./logs/desc/clintox_DESC_20250716.txt\n",
      "python Train_Property_ms.py --dataset hiv --feature DESC --filename 20250716 --project HIV_DESC_20250716  > ./logs/desc/hiv_DESC_20250716.txt\n",
      "python Train_Property_ms.py --dataset freesolv --feature DESC --filename 20250716 --project FREESOLV_DESC_20250716  > ./logs/desc/freesolv_DESC_20250716.txt\n",
      "python Train_Property_ms.py --dataset esol --feature DESC --filename 20250716 --project ESOL_DESC_20250716  > ./logs/desc/esol_DESC_20250716.txt\n",
      "python Train_Property_ms.py --dataset lipophilicity --feature DESC --filename 20250716 --project LIPOPHILICITY_DESC_20250716  > ./logs/desc/lipophilicity_DESC_20250716.txt\n",
      "ls\n"
     ]
    }
   ],
   "source": [
    "ft = 'DESC'\n",
    "for name in ['20250716']:\n",
    "    for dt in ['bace', 'bbbp', 'tox21', 'toxcast', 'sider', 'clintox', 'hiv', 'freesolv', 'esol', 'lipophilicity']:\n",
    "        cmd = f\"python Train_Property_ms.py --dataset {dt} --feature {ft} --filename {name} --project {dt.upper()}_{ft}_{name}  > ./logs/desc/{dt}_{ft}_{name}.txt\"\n",
    "        print(cmd)\n",
    "    print('ls')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geometric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
