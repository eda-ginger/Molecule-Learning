{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rdkit\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from rdkit import Chem, AllChem\n",
    "from deepchem.feat.smiles_tokenizer import BasicSmilesTokenizer\n",
    "from rdkit.Chem import Draw, AllChem, Descriptors, rdDepictor, rdDistGeom, MACCSkeys, rdMolDescriptors\n",
    "from rdkit.Chem import rdDepictor\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric import utils as pyg_utils\n",
    "from torch_geometric.data import InMemoryDataset, download_url, extract_gz, Data, DataLoader, Batch\n",
    "\n",
    "# # 작업을 위한 별도의 함수 불러오기\n",
    "# from utils.download_preprocess import CustomMoleculeNet, atom_features, EDGE_FEATURES\n",
    "\n",
    "# 시각화를 위한 라이브러리\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(rdkit.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CHARSMISET = {\"(\": 1, \".\": 2, \"0\": 3, \"2\": 4, \"4\": 5, \"6\": 6, \"8\": 7, \"@\": 8,\n",
    "                \"B\": 9, \"D\": 10, \"F\": 11, \"H\": 12, \"L\": 13, \"N\": 14, \"P\": 15, \"R\": 16,\n",
    "                \"T\": 17, \"V\": 18, \"Z\": 19, \"\\\\\": 20, \"b\": 21, \"d\": 22, \"f\": 23, \"h\": 24,\n",
    "                \"l\": 25, \"n\": 26, \"r\": 27, \"t\": 28, \"#\": 29, \"%\": 30, \")\": 31, \"+\": 32,\n",
    "                \"-\": 33, \"/\": 34, \"1\": 35, \"3\": 36, \"5\": 37, \"7\": 38, \"9\": 39, \"=\": 40,\n",
    "                \"A\": 41, \"C\": 42, \"E\": 43, \"G\": 44, \"I\": 45, \"K\": 46, \"M\": 47, \"O\": 48,\n",
    "                \"S\": 49, \"U\": 50, \"W\": 51, \"Y\": 52, \"[\": 53, \"]\": 54, \"a\": 55, \"c\": 56,\n",
    "                \"e\": 57, \"g\": 58, \"i\": 59, \"m\": 60, \"o\": 61, \"s\": 62, \"u\": 63, \"y\": 64,\n",
    "                'p': 65, '~': 66, '>': 67, '<': 68} # add p, ~, >, <\n",
    "\n",
    "CHARISOSMILEN = 68\n",
    "\n",
    "CHARPROTSET = {\"A\": 1, \"C\": 2, \"B\": 3, \"E\": 4, \"D\": 5, \"G\": 6,\n",
    "               \"F\": 7, \"I\": 8, \"H\": 9, \"K\": 10, \"M\": 11, \"L\": 12,\n",
    "               \"O\": 13, \"N\": 14, \"Q\": 15, \"P\": 16, \"S\": 17, \"R\": 18,\n",
    "               \"U\": 19, \"T\": 20, \"W\": 21, \"V\": 22, \"Y\": 23, \"X\": 24, \"Z\": 25}\n",
    "\n",
    "CHARPROTLEN = 25\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "########## Function\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "def integer_label_encoding(sequence, tp, max_length=100):\n",
    "    \"\"\"\n",
    "    Integer encoding for string sequence.\n",
    "    Args:\n",
    "        sequence (str): Drug or Protein string sequence.\n",
    "        max_length: Maximum encoding length of input string.\n",
    "    \"\"\"\n",
    "    if tp == 'drug':\n",
    "        charset = CHARSMISET\n",
    "    elif tp == 'protein':\n",
    "        charset = CHARPROTSET\n",
    "\n",
    "    encoding = np.zeros(max_length)\n",
    "    for idx, letter in enumerate(sequence[:max_length]):\n",
    "        try:\n",
    "            if tp == 'protein':\n",
    "                letter = letter.upper()\n",
    "            letter = str(letter)\n",
    "            encoding[idx] = charset[letter]\n",
    "        except KeyError:\n",
    "            print(\n",
    "                f\"character {letter} does not exists in sequence category encoding, skip and treat as padding.\"\n",
    "            )\n",
    "    return Data(x=torch.from_numpy(encoding).to(torch.long).unsqueeze(dim=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit import RDLogger\n",
    "from pathlib import Path\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import add_self_loops\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "RDLogger.DisableLog('rdApp.*')  \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def smiles_to_coord(smiles):\n",
    "    try:\n",
    "        # SMILES → Mol\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            print(f\"[Warning] MolFromSmiles failed for: {smiles}\")\n",
    "            return None\n",
    "        \n",
    "        mol = Chem.AddHs(mol)\n",
    "\n",
    "        # 3D 좌표 생성\n",
    "        status = AllChem.EmbedMolecule(mol, randomSeed=42)\n",
    "        if status == -1:\n",
    "            AllChem.Compute2DCoords(mol)\n",
    "            print(f\"[Warning] EmbedMolecule failed for: {smiles}\")        \n",
    "        else:\n",
    "            AllChem.UFFOptimizeMolecule(mol)\n",
    "\n",
    "        # conformer 가져오기\n",
    "        if mol.GetNumConformers() == 0:\n",
    "            print(f\"[Warning] No conformer generated for: {smiles}\")\n",
    "            return None\n",
    "        conf = mol.GetConformer()\n",
    "\n",
    "        # 원자 번호와 좌표 추출\n",
    "        z = []\n",
    "        pos = []\n",
    "        for atom in mol.GetAtoms():\n",
    "            z.append(atom.GetAtomicNum())\n",
    "            p = conf.GetAtomPosition(atom.GetIdx())\n",
    "            pos.append([p.x, p.y, p.z])\n",
    "        \n",
    "        z = torch.tensor(z, dtype=torch.long)\n",
    "        pos = torch.tensor(pos, dtype=torch.float)\n",
    "        return Data(z=z, pos=pos)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[Exception] Failed for {smiles}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... existing code ...\n",
    "\n",
    "# Updated transform_mol function with progress logging\n",
    "from torch_geometric.data import Data\n",
    "from tqdm import tqdm\n",
    "from utils.molecule_feature import *\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "def transform_mol(molecule_smiles, labels, choice):\n",
    "    mols = [Chem.MolFromSmiles(mol) for mol in molecule_smiles if mol]\n",
    "    print(f\"Processing {len(molecule_smiles)} molecules with {choice} transformation...\")\n",
    "    \n",
    "    # string tokenization\n",
    "    if choice == 'string_tokenization': # vocab dictionary, encoded smiles를 출력\n",
    "        print(\"Building vocabulary from SMILES tokens...\")\n",
    "        vocab = []\n",
    "        max_len = 0\n",
    "        tokenizer = BasicSmilesTokenizer()\n",
    "        for smi in tqdm(molecule_smiles, desc=\"Tokenizing SMILES\"):\n",
    "            tokens = tokenizer.tokenize(smi)\n",
    "            max_len = max(max_len, len(tokens))\n",
    "            vocab += tokens\n",
    "            \n",
    "        uniq_vocab = sorted(set(vocab))\n",
    "        smiles_vocab = {v: i for i, v in enumerate(uniq_vocab)}\n",
    "        smiles_vocab['Unk'] = len(smiles_vocab)\n",
    "        print(f\"Vocabulary size: {len(smiles_vocab)}\")\n",
    "        \n",
    "        print(\"Encoding SMILES sequences...\")\n",
    "        encoded_smiles = [[smiles_vocab.get(token, smiles_vocab['Unk']) for token in tokenizer.tokenize(smi)] for smi in tqdm(molecule_smiles, desc=\"Encoding SMILES\")]\n",
    "        smiles_vec = []\n",
    "        for vec, l, smi in tqdm(zip(encoded_smiles, labels, molecule_smiles), desc=\"Creating Data objects\", total=len(molecule_smiles)):\n",
    "            pad_len = max_len - len(vec)\n",
    "            vec = vec + ([0] * pad_len)\n",
    "            smiles_vec.append(Data(x=torch.tensor(vec).view(1, -1), y=torch.tensor([l], dtype=torch.float).view(1, -1), smiles=smi))\n",
    "        print(f\"Completed string tokenization for {len(smiles_vec)} molecules\")\n",
    "        return smiles_vocab, smiles_vec\n",
    "\n",
    "    # integer encoding (CNN)\n",
    "    elif choice == 'integer_encoding':\n",
    "        print(\"Converting SMILES to integer encoding...\")\n",
    "        integer_encoding_data = []\n",
    "        for smi, l in tqdm(zip(molecule_smiles, labels), desc=\"Integer encoding\", total=len(molecule_smiles)):\n",
    "             drug = integer_label_encoding(smi, 'drug')\n",
    "             drug.y = torch.tensor([l], dtype=torch.float).view(1, -1)\n",
    "             integer_encoding_data.append(drug)\n",
    "        print(f\"Completed integer encoding for {len(integer_encoding_data)} molecules\")\n",
    "        return integer_encoding_data\n",
    "\n",
    "    # 2D Graph\n",
    "    elif choice == '2D_graph':\n",
    "        print(\"Converting SMILES to 2D molecular graphs...\")\n",
    "        graph_data = [smiles_to_feature(smi) for smi in tqdm(molecule_smiles, desc=\"Creating 2D graphs\")]\n",
    "\n",
    "        graph_2d = []\n",
    "        for g, l, smi in tqdm(zip(graph_data, labels, molecule_smiles), desc=\"Adding labels to graphs\", total=len(molecule_smiles)):\n",
    "            g.y = torch.tensor([l], dtype=torch.float).view(1, -1)\n",
    "            g.smiles = smi\n",
    "            graph_2d.append(g)\n",
    "        print(f\"Completed 2D graph conversion for {len(graph_2d)} molecules\")\n",
    "        return graph_2d\n",
    "\n",
    "    # 3D Graph\n",
    "    elif choice == '3D_graph':\n",
    "        print(\"Converting SMILES to 3D molecular graphs...\")\n",
    "        graph_3d = []\n",
    "        for smi, l in tqdm(zip(molecule_smiles, labels), desc=\"Creating 3D graphs\", total=len(molecule_smiles)):\n",
    "            graph_data = smiles_to_coord(smi)\n",
    "            if graph_data is None:\n",
    "                print(f\"Failed to create 3D graph for {smi}\")\n",
    "                continue\n",
    "            graph_data.y = torch.tensor([l], dtype=torch.float).view(1, -1)\n",
    "            graph_3d.append(graph_data)\n",
    "        print(f\"Completed 3D graph conversion for {len(graph_3d)} molecules\")\n",
    "        return graph_3d\n",
    "\n",
    "    # ChemBERTa\n",
    "    elif choice == 'chemberta':\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"DeepChem/ChemBERTa-77M-MLM\")\n",
    "        model = AutoModel.from_pretrained(\"DeepChem/ChemBERTa-77M-MLM\")\n",
    "        print(\"Converting SMILES to ChemBERTa embeddings...\")\n",
    "        chemberta_data = []\n",
    "        for smi, l in tqdm(zip(molecule_smiles, labels), desc=\"Creating ChemBERTa embeddings\", total=len(molecule_smiles)):\n",
    "            with torch.no_grad():\n",
    "                inputs = tokenizer(smi, return_tensors='pt', padding=True, truncation=True)\n",
    "                outputs = model(**inputs)\n",
    "                embedding = outputs.last_hidden_state[:, 0, :].squeeze(0)  # (hidden_size,)\n",
    "            data = Data(x=embedding.unsqueeze(0), smiles=smi)\n",
    "            data.y = torch.tensor([l], dtype=torch.float).view(1, -1)\n",
    "            chemberta_data.append(data)\n",
    "        print(f\"Completed ChemBERTa embedding conversion for {len(chemberta_data)} molecules\")\n",
    "        return chemberta_data\n",
    "    \n",
    "    # Fingerprint\n",
    "    elif 'fingerprint' in choice:\n",
    "        print(f\"Generating {choice} fingerprints...\")\n",
    "        if choice == 'rdkit_fingerprint':\n",
    "            fp = [Chem.RDKFingerprint(mol) for mol in tqdm(mols, desc=\"RDKit fingerprints\")]\n",
    "        \n",
    "        elif choice == 'maccs_fingerprint':\n",
    "            fp = [MACCSkeys.GenMACCSKeys(mol) for mol in tqdm(mols, desc=\"MACCS fingerprints\")]\n",
    "        \n",
    "        elif choice == 'morgan_fingerprint':\n",
    "            fp = [AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=1024) for mol in tqdm(mols, desc=\"Morgan fingerprints\")]\n",
    "\n",
    "        print(\"Converting fingerprints to Data objects...\")\n",
    "        fps = [Data(x=torch.tensor(f).view(1, -1), y=torch.tensor([l], dtype=torch.float).view(1, -1), smiles=smi) for f, l, smi in tqdm(zip(fp, labels, molecule_smiles), desc=\"Creating fingerprint Data objects\", total=len(molecule_smiles))]\n",
    "        print(f\"Completed {choice} generation for {len(fps)} molecules\")\n",
    "        return fps\n",
    "\n",
    "    # Descriptors\n",
    "    elif choice == 'descriptors':\n",
    "        print(\"Calculating molecular descriptors...\")\n",
    "        # 모델 학습을 위해서는 스케일링 작업이 별도로 필요하다는 것을 기억하자!\n",
    "        desc = []\n",
    "        for mol, l, smi in tqdm(zip(mols, labels, molecule_smiles), desc=\"Calculating descriptors\", total=len(molecule_smiles)):\n",
    "            x = torch.tensor(list(Descriptors.CalcMolDescriptors(mol).values()), dtype=torch.float).view(1, -1)\n",
    "            y = torch.tensor([l], dtype=torch.float).view(1, -1)\n",
    "            desc.append(Data(x=x, y=y, smiles=smi))\n",
    "        print(f\"Completed descriptor calculation for {len(desc)} molecules\")\n",
    "        return desc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dta_trn = pd.read_csv('dta_dataset/kiba/train.csv')\n",
    "dta_val = pd.read_csv('dta_dataset/kiba/valid.csv')\n",
    "dta_tst = pd.read_csv('dta_dataset/kiba/test.csv')\n",
    "\n",
    "dta_trn['Set'] = 'TRN'\n",
    "dta_val['Set'] = 'VAL'\n",
    "dta_tst['Set'] = 'TST'\n",
    "\n",
    "dta = pd.concat([dta_trn, dta_val, dta_tst]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dta['CNN'] = transform_mol(dta['Drug'], dta['Y'], 'integer_encoding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dta['2D-GNN'] = transform_mol(dta['Drug'], dta['Y'], '2D_graph')\n",
    "dta['FP-Morgan'] = transform_mol(dta['Drug'], dta['Y'], 'morgan_fingerprint') # 1024\n",
    "dta['FP-MACCS'] = transform_mol(dta['Drug'], dta['Y'], 'maccs_fingerprint') # 167"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dta['ChemBERTa'] = transform_mol(dta['Drug'], dta['Y'], 'chemberta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dta['Target_Rep'] = dta['Target'].apply(lambda x: integer_label_encoding(x, 'protein', 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dta['3D-GNN'] = transform_mol(dta['Drug'], dta['Y'], '3D_graph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "fd = Path('dta_dataset/kiba/feature/')\n",
    "fd.mkdir(parents=True, exist_ok=True)\n",
    "for ft in ['CNN']:\n",
    "# for ft in ['3D-GNN', 'ChemBERTa']:\n",
    "# for ft in ['2D-GNN', 'FP-Morgan', 'FP-MACCS', 'CNN']:\n",
    "    nfd = fd / ft\n",
    "    nfd.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    trn_sub = dta[dta['Set'] == 'TRN'][[ft, 'Target_Rep']].reset_index(drop=True).rename(columns={ft: 'Drug_Rep'}).to_dict('records')\n",
    "    val_sub = dta[dta['Set'] == 'VAL'][[ft, 'Target_Rep']].reset_index(drop=True).rename(columns={ft: 'Drug_Rep'}).to_dict('records')\n",
    "    tst_sub = dta[dta['Set'] == 'TST'][[ft, 'Target_Rep']].reset_index(drop=True).rename(columns={ft: 'Drug_Rep'}).to_dict('records')\n",
    "    # print(f'{ft} feature_dim', dta[ft].values[0].x.shape)\n",
    "    \n",
    "    with open(nfd / 'trn.pkl', 'wb') as f:\n",
    "        pickle.dump(trn_sub, f)\n",
    "    with open(nfd / 'val.pkl', 'wb') as f:\n",
    "        pickle.dump(val_sub, f)\n",
    "    with open(nfd / 'tst.pkl', 'wb') as f:\n",
    "        pickle.dump(tst_sub, f)\n",
    "    \n",
    "    print('Saved', nfd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zinc = pd.read_csv('data/zinc/zinc15_250K.csv')\n",
    "zinc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def transform_mol_nolabel(molecule_smiles, choice):\n",
    "#     mols = [Chem.MolFromSmiles(mol) for mol in molecule_smiles if mol]\n",
    "#     print(f\"Processing {len(molecule_smiles)} molecules with {choice} transformation...\")\n",
    "    \n",
    "#      # integer encoding (CNN)\n",
    "#     if choice == 'integer_encoding':\n",
    "#         print(\"Converting SMILES to integer encoding...\")\n",
    "#         integer_encoding_data = {}\n",
    "#         for smi in tqdm(molecule_smiles):\n",
    "#              drug = integer_label_encoding(smi, 'drug')\n",
    "#              integer_encoding_data[smi] = drug\n",
    "#         print(f\"Completed integer encoding for {len(integer_encoding_data)} molecules\")\n",
    "#         return integer_encoding_data\n",
    "\n",
    "#     # 2D Graph\n",
    "#     elif choice == '2D_graph':\n",
    "#         print(\"Converting SMILES to 2D molecular graphs...\")\n",
    "#         graph_data = {smi: drug_to_graph(smi) for smi in tqdm(molecule_smiles, desc=\"Creating 2D graphs\")}\n",
    "#         print(f\"Completed 2D graph conversion for {len(graph_data)} molecules\")\n",
    "#         return graph_data\n",
    "\n",
    "#     # 3D Graph\n",
    "#     elif choice == '3D_graph':\n",
    "#         print(\"Converting SMILES to 3D molecular graphs...\")\n",
    "#         graph_3d = {}\n",
    "#         for smi in tqdm(molecule_smiles):\n",
    "#             graph_data = drug_to_graph(smi)\n",
    "            \n",
    "#             mol = Chem.MolFromSmiles(smi)\n",
    "#             atom_info = [(atom.GetIdx(), atom.GetSymbol()) for atom in mol.GetAtoms()]\n",
    "                     \n",
    "#             mol = AllChem.AddHs(mol, addCoords=True)\n",
    "#             emb_mol = rdDistGeom.EmbedMolecule(mol)\n",
    "#             if emb_mol == -1:\n",
    "#                 rdDepictor.Compute2DCoords(mol)\n",
    "\n",
    "#             conf = mol.GetConformer()\n",
    "#             pos = np.array([conf.GetAtomPosition(idx) for idx, symbol in atom_info])\n",
    "#             graph_data.pos = pos\n",
    "#             graph_3d[smi] = graph_data\n",
    "#         print(f\"Completed 3D graph conversion for {len(graph_3d)} molecules\")\n",
    "#         return graph_3d\n",
    "    \n",
    "#     # Fingerprint\n",
    "#     elif 'fingerprint' in choice:\n",
    "#         print(f\"Generating {choice} fingerprints...\")\n",
    "#         if choice == 'rdkit_fingerprint':\n",
    "#             fp = [Chem.RDKFingerprint(mol) for mol in tqdm(mols, desc=\"RDKit fingerprints\")]\n",
    "        \n",
    "#         elif choice == 'maccs_fingerprint':\n",
    "#             fp = [MACCSkeys.GenMACCSKeys(mol) for mol in tqdm(mols, desc=\"MACCS fingerprints\")]\n",
    "        \n",
    "#         elif choice == 'morgan_fingerprint':\n",
    "#             fp = [AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=1024) for mol in tqdm(mols, desc=\"Morgan fingerprints\")]\n",
    "\n",
    "#         print(\"Converting fingerprints to Data objects...\")\n",
    "#         fps = {smi: Data(x=torch.tensor(f).view(1, -1)) for f, smi in tqdm(zip(fp, molecule_smiles), desc=\"Creating fingerprint Data objects\")}\n",
    "#         print(f\"Completed {choice} generation for {len(fps)} molecules\")\n",
    "#         return fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = '20250628'\n",
    "# for dt in ['bace', 'bbbp', 'tox21', 'toxcast', 'sider', 'clintox', 'hiv']:\n",
    "for dt in ['tox21', 'hiv']:\n",
    "    for ft in ['ChemBERTa']:\n",
    "        cmd = f\"python Train_Property.py --dataset {dt} --feature {ft} --filename {name} --project {dt.upper()}_{ft}_{name}  > ./logs/new/{dt}_{ft}.txt\"\n",
    "        print(cmd)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = '20250710'\n",
    "for dt in ['bace', 'bbbp', 'tox21', 'toxcast', 'sider', 'clintox', 'hiv', 'esol', 'freesolv', 'lipophilicity']:\n",
    "    for ft in ['2D-GNN', '2D-GNN-tuto', 'CNN', 'FP-MACCS', 'FP-Morgan', '3D-GNN', 'ChemBERTa']:\n",
    "        cmd = f\"python Train_Property.py --dataset {dt} --feature {ft} --filename {name} --project {dt.upper()}_{ft}_{name}  > ./logs/new2/{dt}_{ft}.txt\"\n",
    "        print(cmd)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = '20250630'\n",
    "for dt in ['bace', 'bbbp', 'tox21', 'toxcast', 'sider', 'clintox', 'hiv']:\n",
    "    for ft in ['2D-GNN', '2D-GNN-tuto', 'CNN', 'FP-MACCS', 'FP-Morgan', '3D-GNN', 'ChemBERTa']:\n",
    "        cmd = f\"python Train_Property.py --dataset {dt} --feature {ft} --filename {name} --project {dt.upper()}_{ft}_{name}  > ./logs/new2/{dt}_{ft}.txt\"\n",
    "        print(cmd)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.loader import MoleculeDataset ##\n",
    "data_root = \"dataset/\"\n",
    "feature= '2D-GNN'\n",
    "\n",
    "dataset = MoleculeDataset(data_root + 'bace', dataset='bace', feature=feature)\n",
    "dataset = MoleculeDataset(data_root + 'bbbp', dataset='bbbp', feature=feature)\n",
    "dataset = MoleculeDataset(data_root + 'tox21', dataset='tox21', feature=feature)\n",
    "dataset = MoleculeDataset(data_root + 'toxcast', dataset='toxcast', feature=feature)\n",
    "dataset = MoleculeDataset(data_root + 'sider', dataset='sider', feature=feature)\n",
    "dataset = MoleculeDataset(data_root + 'clintox', dataset='clintox', feature=feature)\n",
    "dataset = MoleculeDataset(data_root + 'hiv', dataset='hiv', feature=feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dt_name in ['bace', 'bbbp', 'tox21', 'toxcast', 'sider', 'clintox', 'hiv']:\n",
    "    print(dt_name)\n",
    "    dt1 = MoleculeDataset(data_root + dt_name, dataset=dt_name, feature='2D-GNN')\n",
    "    dt2 = MoleculeDataset(data_root + dt_name, dataset=dt_name, feature='3D-GNN')\n",
    "    check = set(dt1.smiles) & set(dt2.smiles)\n",
    "    print(len(dt1.smiles), len(dt2.smiles), len(check))\n",
    "    if len(check) != len(dt1.smiles):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.loader import MoleculeDataset ##\n",
    "data_root = \"dataset/\"\n",
    "for feature in ['DESC']:\n",
    "    dataset = MoleculeDataset(data_root + 'bace', dataset='bace', feature=feature)\n",
    "    dataset = MoleculeDataset(data_root + 'bbbp', dataset='bbbp', feature=feature)\n",
    "    dataset = MoleculeDataset(data_root + 'tox21', dataset='tox21', feature=feature)\n",
    "    dataset = MoleculeDataset(data_root + 'toxcast', dataset='toxcast', feature=feature)\n",
    "    dataset = MoleculeDataset(data_root + 'sider', dataset='sider', feature=feature)\n",
    "    dataset = MoleculeDataset(data_root + 'clintox', dataset='clintox', feature=feature)\n",
    "    dataset = MoleculeDataset(data_root + 'hiv', dataset='hiv', feature=feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.loader import MoleculeDataset ##\n",
    "data_root = \"dataset/\"\n",
    "for dt_name in ['bace', 'bbbp', 'tox21', 'toxcast', 'sider', 'clintox', 'hiv']:\n",
    "    dataset_base = MoleculeDataset(data_root + dt_name, dataset=dt_name, feature='2D-GNN')\n",
    "    dataset_3d = MoleculeDataset(data_root + dt_name, dataset=dt_name, feature='3D-GNN')\n",
    "\n",
    "    base_smi = '\\n'.join(str(i) for i in dataset_base.smiles)\n",
    "    with open(f'dataset/{dt_name}/processed/base_smi.txt', 'w') as f:\n",
    "        f.write(f'base_smi: {len(dataset_base.smiles)}\\n')\n",
    "        f.write(base_smi)\n",
    "\n",
    "    smi_3d = set(dataset_base.smiles) & set(dataset_3d.smiles)\n",
    "    smi_3d_file = '\\n'.join(str(i) for i in smi_3d)\n",
    "    with open(f'dataset/{dt_name}/processed/3d_smi.txt', 'w') as f:\n",
    "        f.write(f'3d_smi: {len(smi_3d)}\\n')\n",
    "        f.write(smi_3d_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.loader import MoleculeDataset ##\n",
    "data_root = \"dataset/\"\n",
    "# for feature in ['3D-GNN']:\n",
    "for feature in ['2D-GNN', '2D-GNN-tuto', 'CNN', '3D-GNN','FP-MACCS', 'FP-Morgan', 'ChemBERTa']:\n",
    "    print(feature)\n",
    "    dataset = MoleculeDataset(data_root + 'esol', dataset='esol', feature=feature)\n",
    "    dataset = MoleculeDataset(data_root + 'freesolv', dataset='freesolv', feature=feature)\n",
    "    dataset = MoleculeDataset(data_root + 'lipophilicity', dataset='lipophilicity', feature=feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = '20250708'\n",
    "# for dt in ['bace', 'bbbp', 'tox21', 'toxcast', 'sider', 'clintox', 'hiv', 'freesolv', 'esol', 'lipophilicity']:\n",
    "for ft in ['2D-GNN-tuto']:\n",
    "    for dt in ['hiv']:\n",
    "        for tp in ['2L-GCN']:\n",
    "            cmd = f\"python Train_Property.py --dataset {dt} --feature {ft} --filename {name}-{tp} --project {dt.upper()}_{ft}_{name}_{tp}  > ./logs/comp_gcl_tuto_others/{dt}_{ft}_{tp}.txt\"\n",
    "            print(cmd)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = '20250708'\n",
    "for ft in []:\n",
    "    for dt in ['bace', 'bbbp', 'tox21', 'toxcast', 'sider', 'hiv', 'freesolv', 'esol', 'lipophilicity']:\n",
    "        \n",
    "            cmd = f\"python Train_Property.py --dataset {dt} --feature {ft} --filename {name}-{tp} --project {dt.upper()}_{ft}_{name}_{tp}  > ./logs/comp_gcl_tuto_others/{dt}_{ft}_{tp}.txt\"\n",
    "                print(cmd)\n",
    "        if ft == '2D-GNN-copy':\n",
    "            for tp in ['2L-GIN', '5L-GIN', '2L-GCN', '5L-GCN', '2L-GIN-emb', '5L-GIN-emb', '2L-GCN-emb', '5L-GCN-emb', '2L-GIN-emb-fit', '5L-GIN-emb-fit', '2L-GCN-emb-fit', '5L-GCN-emb-fit']:\n",
    "                cmd = f\"python Train_Property.py --dataset {dt} --feature {ft} --filename {name}-{tp} --project {dt.upper()}_{ft}_{name}_{tp}  > ./logs/comp_gcl_tuto_others/{dt}_{ft}_{tp}.txt\"\n",
    "                print(cmd)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = '20250708'\n",
    "# for dt in ['bace', 'bbbp', 'tox21', 'toxcast', 'sider', 'clintox', 'hiv', 'freesolv', 'esol', 'lipophilicity']:\n",
    "for ft in ['2D-GNN-copy']:\n",
    "    for dt in ['bace', 'bbbp', 'tox21', 'toxcast', 'sider', 'hiv', 'freesolv', 'esol', 'lipophilicity']:\n",
    "        if ft in ['2D-GNN-tuto', '2D-GNN-copy2', '2D-GNN-copy3']:\n",
    "            for tp in ['2L-GIN', '5L-GIN', '2L-GCN', '5L-GCN']:\n",
    "                cmd = f\"python Train_Property.py --dataset {dt} --feature {ft} --filename {name}-{tp} --project {dt.upper()}_{ft}_{name}_{tp}  > ./logs/comp_gcl_tuto_others/{dt}_{ft}_{tp}.txt\"\n",
    "                print(cmd)\n",
    "        if ft == '2D-GNN-copy':\n",
    "            for tp in ['2L-GIN', '5L-GIN', '2L-GCN', '5L-GCN', '2L-GIN-emb', '5L-GIN-emb', '2L-GCN-emb', '5L-GCN-emb', '2L-GIN-emb-fit', '5L-GIN-emb-fit', '2L-GCN-emb-fit', '5L-GCN-emb-fit']:\n",
    "                cmd = f\"python Train_Property.py --dataset {dt} --feature {ft} --filename {name}-{tp} --project {dt.upper()}_{ft}_{name}_{tp}  > ./logs/comp_gcl_tuto_others/{dt}_{ft}_{tp}.txt\"\n",
    "                print(cmd)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for dt in ['bace', 'bbbp', 'tox21', 'toxcast', 'sider', 'clintox', 'hiv', 'freesolv', 'esol', 'lipophilicity']:\n",
    "    for ft in ['DESC']:\n",
    "        \n",
    "        cmd = f\"python Train_Property.py --dataset {dt} --feature {ft} --filename {name} --project {dt.upper()}_{ft}_{name}  > ./logs/new4/{dt}_{ft}.txt\"\n",
    "        print(cmd)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.loader import MoleculeDataset ##\n",
    "data_root = \"dataset/\"\n",
    "for dt_name in ['freesolv', 'esol', 'lipophilicity']:\n",
    "    dataset_base = MoleculeDataset(data_root + dt_name, dataset=dt_name, feature='2D-GNN')\n",
    "    dataset_3d = MoleculeDataset(data_root + dt_name, dataset=dt_name, feature='3D-GNN')\n",
    "\n",
    "    base_smi = '\\n'.join(str(i) for i in dataset_base.smiles)\n",
    "    with open(f'dataset/{dt_name}/processed/base_smi.txt', 'w') as f:\n",
    "        f.write(f'base_smi: {len(dataset_base.smiles)}\\n')\n",
    "        f.write(base_smi)\n",
    "\n",
    "    smi_3d = set(dataset_base.smiles) & set(dataset_3d.smiles)\n",
    "    smi_3d_file = '\\n'.join(str(i) for i in smi_3d)\n",
    "    with open(f'dataset/{dt_name}/processed/3d_smi.txt', 'w') as f:\n",
    "        f.write(f'3d_smi: {len(smi_3d)}\\n')\n",
    "        f.write(smi_3d_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "folder = Path('logs/comp_gcl_tuto_others')\n",
    "result = []\n",
    "for file in folder.glob('*.txt'):\n",
    "    fn = file.stem\n",
    "    dn, ftn, mdn = fn.split('_')\n",
    "    \n",
    "    if ftn == '2D-GNN-copy':\n",
    "        mdnn = mdn.split('-')\n",
    "        # print(mdnn)\n",
    "        if mdnn[-1] == 'fit':\n",
    "            pn = 5\n",
    "        elif mdnn[-1] == 'emb':\n",
    "            pn = 4\n",
    "        else:\n",
    "            pn = 3\n",
    "    elif ftn == '2D-GNN-copy2':\n",
    "        pn = 2\n",
    "    elif ftn == '2D-GNN-copy3':\n",
    "        pn = 1\n",
    "    elif ftn == '2D-GNN-tuto':\n",
    "        pn = 0\n",
    "    \n",
    "    print(ftn, mdn, pn)\n",
    "\n",
    "    with open(file, 'r', encoding='utf-16') as f:\n",
    "        lines = f.readlines()\n",
    "        if 'Test Score' in lines[-1]:\n",
    "            result.append({'dataset': dn, 'ft': ftn, 'md': mdn, 'pn': pn, 'score': lines[-1].split(':')[-1].strip()})\n",
    "\n",
    "pd.DataFrame(result).to_excel('summary_results.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = '20250712'\n",
    "for ft in ['FP-MACCS', 'FP-Morgan', 'CNN', 'ChemBERTa', '2D-GNN-tuto', '3D-GNN']:\n",
    "    for dt in ['bace', 'bbbp', 'tox21', 'toxcast', 'sider', 'hiv', 'freesolv', 'esol', 'lipophilicity']:\n",
    "        cmd = f\"python Train_Property_ms.py --dataset {dt} --feature {ft} --filename {name} --project {dt.upper()}_{ft}_{name}  > ./logs/{name}/{dt}_{ft}.txt\"\n",
    "        print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from tqdm import tqdm\n",
    "from rdkit import RDLogger\n",
    "from rdkit.Chem import Descriptors\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "def calc_desc(smi):\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    desc = Descriptors.CalcMolDescriptors(mol)\n",
    "    if desc:\n",
    "        if pd.DataFrame([desc]).isnull().sum().sum() == len(desc):\n",
    "            return None\n",
    "        else:\n",
    "            desc_select = {}\n",
    "            for k, v in desc.items():\n",
    "                if k.startswith('fr'):\n",
    "                    desc_select[k] = v\n",
    "                elif k.startswith('Num') or k.endswith('Count'):\n",
    "                    desc_select[k] = v\n",
    "                elif k in ['qed', 'SPS', 'ExactMolWt', 'MolWt', 'TPSA', 'HeavyAtomMolWt', 'Ipc', 'MolLogP', 'MolMR', 'HallKierAlpha', 'FractionCSP3']:\n",
    "                    desc_select[k] = v\n",
    "            return desc_select\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from rdkit.Chem import AllChem\n",
    "from data.loader import MoleculeDataset\n",
    "from data.splitters import scaffold_split\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "total = []\n",
    "fds = [fd for fd in Path('dataset').glob('*')]\n",
    "for fd in fds:\n",
    "    data = pd.read_csv([f for f in (fd / 'raw').glob('*.csv')][0])\n",
    "\n",
    "    data_root = \"dataset/\"\n",
    "    dataset = MoleculeDataset(data_root + fd.stem, dataset=fd.stem, feature='CNN')\n",
    "    smiles_list = pd.read_csv(data_root + fd.stem + '/processed/smiles.csv', header=None)[0].tolist()\n",
    "    train_dataset, valid_dataset, test_dataset = scaffold_split(dataset, smiles_list, null_value=0, frac_train=0.8,frac_valid=0.1, frac_test=0.1)\n",
    "    \n",
    "    trn_smiles = sum(train_dataset.smiles, [])\n",
    "    val_smiles = sum(valid_dataset.smiles, [])\n",
    "    tst_smiles = sum(test_dataset.smiles, [])\n",
    "\n",
    "    trn_y = {i.smiles[0]: ','.join(map(str, i.y.tolist())) for i in train_dataset}\n",
    "    val_y = {i.smiles[0]: ','.join(map(str, i.y.tolist())) for i in valid_dataset}\n",
    "    tst_y = {i.smiles[0]: ','.join(map(str, i.y.tolist())) for i in test_dataset}\n",
    "    \n",
    "    total_y = {}\n",
    "    total_y.update(trn_y)\n",
    "    total_y.update(val_y)\n",
    "    total_y.update(tst_y)\n",
    "    \n",
    "    if fd.stem == 'bace':\n",
    "        smi_col = 'mol'\n",
    "    else:\n",
    "        smi_col = 'smiles'\n",
    "\n",
    "    box = {'trn': {}, 'val': {}, 'tst': {}}\n",
    "    fail = []\n",
    "    exclude = []\n",
    "    for smi in tqdm(data[smi_col]):\n",
    "        \n",
    "        desc = calc_desc(smi)\n",
    "        if desc:\n",
    "            if fd.stem in ['bbbp', 'toxcast', 'clintox']:\n",
    "                mol = AllChem.MolFromSmiles(smi)\n",
    "                smi = AllChem.MolToSmiles(mol)\n",
    "            \n",
    "            desc['smiles'] = smi\n",
    "            if smi in trn_smiles:\n",
    "                box['trn'][smi] = desc\n",
    "            elif smi in val_smiles:\n",
    "                box['val'][smi] = desc\n",
    "            elif smi in tst_smiles:\n",
    "                box['tst'][smi] = desc\n",
    "            else:\n",
    "                exclude.append(smi)\n",
    "        else:\n",
    "            fail.append(smi)\n",
    "    \n",
    "    trn = pd.DataFrame(box['trn'].values())\n",
    "    val = pd.DataFrame(box['val'].values())\n",
    "    tst = pd.DataFrame(box['tst'].values())\n",
    "    \n",
    "    trn_sub = copy.deepcopy(trn)\n",
    "    trn_sub['set'] = 'train'\n",
    "    \n",
    "    val_sub = copy.deepcopy(val)\n",
    "    val_sub['set'] = 'valid'\n",
    "    \n",
    "    tst_sub = copy.deepcopy(tst)\n",
    "    tst_sub['set'] = 'test'\n",
    "    \n",
    "    pre_total = pd.concat([trn_sub, val_sub, tst_sub]).reset_index(drop=True)\n",
    "    pre_total['y'] = pre_total['smiles'].map(total_y)\n",
    "    pre_total.to_csv(fd / 'processed' / 'desc_pre.csv', index=False)\n",
    "    \n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "    trn[trn.columns[:-1]] = pd.DataFrame(scaler.fit_transform(trn[trn.columns[:-1]]), columns=trn.columns[:-1])\n",
    "    val[val.columns[:-1]] = pd.DataFrame(scaler.transform(val[val.columns[:-1]]), columns=val.columns[:-1])\n",
    "    tst[tst.columns[:-1]] = pd.DataFrame(scaler.transform(tst[tst.columns[:-1]]), columns=tst.columns[:-1])\n",
    "    \n",
    "    total = pd.concat([trn, val, tst]).reset_index(drop=True)\n",
    "\n",
    "    result = {}\n",
    "    for _, row in total.iterrows():\n",
    "        result[row['smiles']] = torch.tensor(list(row.values[:-1])).unsqueeze(0)\n",
    "\n",
    "    with open(fd / 'processed' / 'desc.pkl', 'wb') as f:\n",
    "        pickle.dump(result, f)\n",
    "    \n",
    "    print(fd.stem, len(result), len(fail), len(exclude))\n",
    "    print(result[row['smiles']].shape)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.loader import MoleculeDataset\n",
    "data_root = \"dataset/\"\n",
    "for feature in ['DESC']:\n",
    "    dataset = MoleculeDataset(data_root + 'bace', dataset='bace', feature=feature)\n",
    "    dataset = MoleculeDataset(data_root + 'bbbp', dataset='bbbp', feature=feature)\n",
    "    dataset = MoleculeDataset(data_root + 'tox21', dataset='tox21', feature=feature)\n",
    "    dataset = MoleculeDataset(data_root + 'toxcast', dataset='toxcast', feature=feature)\n",
    "    dataset = MoleculeDataset(data_root + 'sider', dataset='sider', feature=feature)\n",
    "    dataset = MoleculeDataset(data_root + 'clintox', dataset='clintox', feature=feature)\n",
    "    dataset = MoleculeDataset(data_root + 'hiv', dataset='hiv', feature=feature)\n",
    "    dataset = MoleculeDataset(data_root + 'freesolv', dataset='freesolv', feature=feature)\n",
    "    dataset = MoleculeDataset(data_root + 'esol', dataset='esol', feature=feature)\n",
    "    dataset = MoleculeDataset(data_root + 'lipophilicity', dataset='lipophilicity', feature=feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from data.loader import MoleculeDataset\n",
    "from data.splitters import scaffold_split\n",
    "\n",
    "data_root = \"dataset/\"\n",
    "dataset = MoleculeDataset(data_root + 'bbbp', dataset='bbbp', feature='DESC')\n",
    "smiles_list = pd.read_csv(data_root + 'bbbp' + '/processed/smiles.csv', header=None)[0].tolist()\n",
    "train_dataset, valid_dataset, test_dataset = scaffold_split(dataset, smiles_list, null_value=0, frac_train=0.8,frac_valid=0.1, frac_test=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = 'DESC'\n",
    "for name in ['20250716']:\n",
    "    for dt in ['bace', 'bbbp', 'tox21', 'toxcast', 'sider', 'clintox', 'hiv', 'freesolv', 'esol', 'lipophilicity']:\n",
    "        cmd = f\"python Train_Property_ms.py --dataset {dt} --feature {ft} --filename {name} --project {dt.upper()}_{ft}_{name}  > ./logs/desc/{dt}_{ft}_{name}.txt\"\n",
    "        print(cmd)\n",
    "    print('ls')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.DataFrame(torch.load('llm/gemma_bace_desc_w.pt'))\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['last_hidden_state'].values[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.loader import MoleculeDataset\n",
    "data_root = \"dataset/\"\n",
    "for feature in ['gemma-mean', 'gemma-max', 'gemma-last']:\n",
    "    # dataset = MoleculeDataset(data_root + 'bace', dataset='bace', feature=feature)\n",
    "    # dataset = MoleculeDataset(data_root + 'bbbp', dataset='bbbp', feature=feature)\n",
    "    # dataset = MoleculeDataset(data_root + 'esol', dataset='esol', feature=feature)\n",
    "    dataset = MoleculeDataset(data_root + 'lipophilicity', dataset='lipophilicity', feature=feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.data.x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '20250724'\n",
    "# for ft in ['gemma-desc-mean', 'gemma-desc-max', 'gemma-desc-last']:\n",
    "for ft in ['gemma-desc-mean', 'gemma-desc-max', 'gemma-desc-last']:\n",
    "    for dt in ['bace', 'bbbp', 'esol', 'lipo']:\n",
    "        if dt == 'lipo':\n",
    "            cmd = f\"python Train_Property_ms.py --dataset lipophilicity --feature {ft} --filename {filename} --project {dt.upper()}_{ft}_{filename}  > ./logs/llm/{dt}_{ft}_{filename}.txt\"\n",
    "        else:\n",
    "            cmd = f\"python Train_Property_ms.py --dataset {dt} --feature {ft} --filename {filename} --project {dt.upper()}_{ft}_{filename}  > ./logs/llm/{dt}_{ft}_{filename}.txt\"\n",
    "        print(cmd)\n",
    "print('ls')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.randn(1, 1280, 2560).max(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152\n",
      "gemma_bace: 0.5328947368421053, 0.5\n",
      "198\n",
      "gemma_bbbp: 0.5757575757575758, 0.5417479716545137\n",
      "113\n",
      "gemma_esol: 85.48852253124522\n",
      "420\n",
      "gemma_lipo: 1.7495105367204715\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from utils.splitters import scaffold_split\n",
    "from data.loader import MoleculeDataset\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score, accuracy_score\n",
    "\n",
    "data_root = \"dataset/\"\n",
    "for model in ['gemma']:\n",
    "    for task in ['bace', 'bbbp', 'esol', 'lipo']:\n",
    "        if task == 'lipo':\n",
    "            dataset = MoleculeDataset(data_root + 'lipophilicity', dataset='lipophilicity', feature='CNN')\n",
    "            smiles_list = pd.read_csv(data_root + 'lipophilicity' + '/processed/smiles.csv', header=None)[0].tolist()\n",
    "            train_dataset, valid_dataset, test_dataset = scaffold_split(dataset, smiles_list, null_value=0, frac_train=0.8,frac_valid=0.1, frac_test=0.1)\n",
    "        else:\n",
    "            dataset = MoleculeDataset(data_root + task, dataset=task, feature='CNN')\n",
    "            smiles_list = pd.read_csv(data_root + task + '/processed/smiles.csv', header=None)[0].tolist()\n",
    "            train_dataset, valid_dataset, test_dataset = scaffold_split(dataset, smiles_list, null_value=0, frac_train=0.8,frac_valid=0.1, frac_test=0.1)\n",
    "        \n",
    "        smis = [s[0] for s in test_dataset.smiles]\n",
    "\n",
    "        # data = torch.load(f'output/{model}_{task}_desc_wo.pt')\n",
    "        data = pd.DataFrame(torch.load(f'llm/{model}_{task}_desc_w.pt'))\n",
    "        data = data[data['smiles'].isin(smis)]\n",
    "        print(len(data))\n",
    "        \n",
    "        if task in ['bace', 'bbbp']:\n",
    "            data = data[['y', 'chat']].astype(int).replace(0, -1)\n",
    "            acc = accuracy_score(data['y'], data['chat'])\n",
    "            roc = roc_auc_score(data['y'], data['chat'])\n",
    "            print(f'{model}_{task}: {acc}, {roc}')\n",
    "        else:\n",
    "            data = data[['y', 'chat']].astype(float)\n",
    "            mse = np.sqrt(mean_squared_error(data['y'], data['chat']))\n",
    "            print(f'{model}_{task}: {mse}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  0.2178,   0.3203,  -0.0942,  ...,  -0.1611,   0.0938,   0.3750],\n",
       "         [  0.2373,  -0.9102,   0.2285,  ...,  -0.9414,  -0.1016,   0.4648],\n",
       "         [  0.2754,   3.2344,  -1.2812,  ...,   0.8086,  -3.1094,   0.8398],\n",
       "         ...,\n",
       "         [  4.4062,  -2.8594,   0.8750,  ...,  -3.4531, -10.6875,   4.6875],\n",
       "         [  8.3750,  -0.8008,  -0.2002,  ...,   0.1797,  -7.9688,  -1.9297],\n",
       "         [  3.0938,  -1.5312,  -1.6641,  ...,  -1.3828,  -3.4688,  -0.2832]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(torch.load(f'llm/{model}_{task}_desc_w.pt'))\n",
    "data['last_hidden_state'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 339, 2560])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['last_hidden_state'].values[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(torch.load(f'llm/{model}_{task}_desc_w.pt'))\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "data = pd.DataFrame(torch.load(f'llm/gemma_lipo_desc_w.pt'))\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.loader import MoleculeDataset\n",
    "data_root = \"dataset/\"\n",
    "for feature in ['gemma-desc-mean', 'gemma-desc-max', 'gemma-desc-last']:\n",
    "    dataset = MoleculeDataset(data_root + 'bace', dataset='bace', feature=feature)\n",
    "    dataset = MoleculeDataset(data_root + 'bbbp', dataset='bbbp', feature=feature)\n",
    "    dataset = MoleculeDataset(data_root + 'esol', dataset='esol', feature=feature)\n",
    "    dataset = MoleculeDataset(data_root + 'lipophilicity', dataset='lipophilicity', feature=feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python Train_Property_ms.py --dataset bace --feature gemma-desc-mean --filename deep --project BACE_gemma-desc-mean_deep  > ./logs/deepnet/bace_gemma-desc-mean_deep.txt\n",
      "python Train_Property_ms.py --dataset bbbp --feature gemma-desc-mean --filename deep --project BBBP_gemma-desc-mean_deep  > ./logs/deepnet/bbbp_gemma-desc-mean_deep.txt\n",
      "python Train_Property_ms.py --dataset esol --feature gemma-desc-mean --filename deep --project ESOL_gemma-desc-mean_deep  > ./logs/deepnet/esol_gemma-desc-mean_deep.txt\n",
      "python Train_Property_ms.py --dataset lipophilicity --feature gemma-desc-mean --filename deep --project LIPOPHILICITY_gemma-desc-mean_deep  > ./logs/deepnet/lipophilicity_gemma-desc-mean_deep.txt\n",
      "python Train_Property_ms.py --dataset bace --feature gemma-desc-max --filename deep --project BACE_gemma-desc-max_deep  > ./logs/deepnet/bace_gemma-desc-max_deep.txt\n",
      "python Train_Property_ms.py --dataset bbbp --feature gemma-desc-max --filename deep --project BBBP_gemma-desc-max_deep  > ./logs/deepnet/bbbp_gemma-desc-max_deep.txt\n",
      "python Train_Property_ms.py --dataset esol --feature gemma-desc-max --filename deep --project ESOL_gemma-desc-max_deep  > ./logs/deepnet/esol_gemma-desc-max_deep.txt\n",
      "python Train_Property_ms.py --dataset lipophilicity --feature gemma-desc-max --filename deep --project LIPOPHILICITY_gemma-desc-max_deep  > ./logs/deepnet/lipophilicity_gemma-desc-max_deep.txt\n",
      "python Train_Property_ms.py --dataset bace --feature gemma-desc-last --filename deep --project BACE_gemma-desc-last_deep  > ./logs/deepnet/bace_gemma-desc-last_deep.txt\n",
      "python Train_Property_ms.py --dataset bbbp --feature gemma-desc-last --filename deep --project BBBP_gemma-desc-last_deep  > ./logs/deepnet/bbbp_gemma-desc-last_deep.txt\n",
      "python Train_Property_ms.py --dataset esol --feature gemma-desc-last --filename deep --project ESOL_gemma-desc-last_deep  > ./logs/deepnet/esol_gemma-desc-last_deep.txt\n",
      "python Train_Property_ms.py --dataset lipophilicity --feature gemma-desc-last --filename deep --project LIPOPHILICITY_gemma-desc-last_deep  > ./logs/deepnet/lipophilicity_gemma-desc-last_deep.txt\n",
      "python Train_Property_ms.py --dataset bace --feature gemma-mean --filename deep --project BACE_gemma-mean_deep  > ./logs/deepnet/bace_gemma-mean_deep.txt\n",
      "python Train_Property_ms.py --dataset bbbp --feature gemma-mean --filename deep --project BBBP_gemma-mean_deep  > ./logs/deepnet/bbbp_gemma-mean_deep.txt\n",
      "python Train_Property_ms.py --dataset esol --feature gemma-mean --filename deep --project ESOL_gemma-mean_deep  > ./logs/deepnet/esol_gemma-mean_deep.txt\n",
      "python Train_Property_ms.py --dataset lipophilicity --feature gemma-mean --filename deep --project LIPOPHILICITY_gemma-mean_deep  > ./logs/deepnet/lipophilicity_gemma-mean_deep.txt\n",
      "python Train_Property_ms.py --dataset bace --feature gemma-max --filename deep --project BACE_gemma-max_deep  > ./logs/deepnet/bace_gemma-max_deep.txt\n",
      "python Train_Property_ms.py --dataset bbbp --feature gemma-max --filename deep --project BBBP_gemma-max_deep  > ./logs/deepnet/bbbp_gemma-max_deep.txt\n",
      "python Train_Property_ms.py --dataset esol --feature gemma-max --filename deep --project ESOL_gemma-max_deep  > ./logs/deepnet/esol_gemma-max_deep.txt\n",
      "python Train_Property_ms.py --dataset lipophilicity --feature gemma-max --filename deep --project LIPOPHILICITY_gemma-max_deep  > ./logs/deepnet/lipophilicity_gemma-max_deep.txt\n",
      "ls\n",
      "python Train_Property_ms.py --dataset bace --feature gemma-last --filename deep --project BACE_gemma-last_deep  > ./logs/deepnet/bace_gemma-last_deep.txt\n",
      "python Train_Property_ms.py --dataset bbbp --feature gemma-last --filename deep --project BBBP_gemma-last_deep  > ./logs/deepnet/bbbp_gemma-last_deep.txt\n",
      "python Train_Property_ms.py --dataset esol --feature gemma-last --filename deep --project ESOL_gemma-last_deep  > ./logs/deepnet/esol_gemma-last_deep.txt\n",
      "python Train_Property_ms.py --dataset lipophilicity --feature gemma-last --filename deep --project LIPOPHILICITY_gemma-last_deep  > ./logs/deepnet/lipophilicity_gemma-last_deep.txt\n",
      "python Train_Property_ms.py --dataset bace --feature FP-MACCS --filename deep --project BACE_FP-MACCS_deep  > ./logs/deepnet/bace_FP-MACCS_deep.txt\n",
      "python Train_Property_ms.py --dataset bbbp --feature FP-MACCS --filename deep --project BBBP_FP-MACCS_deep  > ./logs/deepnet/bbbp_FP-MACCS_deep.txt\n",
      "python Train_Property_ms.py --dataset tox21 --feature FP-MACCS --filename deep --project TOX21_FP-MACCS_deep  > ./logs/deepnet/tox21_FP-MACCS_deep.txt\n",
      "python Train_Property_ms.py --dataset toxcast --feature FP-MACCS --filename deep --project TOXCAST_FP-MACCS_deep  > ./logs/deepnet/toxcast_FP-MACCS_deep.txt\n",
      "python Train_Property_ms.py --dataset sider --feature FP-MACCS --filename deep --project SIDER_FP-MACCS_deep  > ./logs/deepnet/sider_FP-MACCS_deep.txt\n",
      "python Train_Property_ms.py --dataset clintox --feature FP-MACCS --filename deep --project CLINTOX_FP-MACCS_deep  > ./logs/deepnet/clintox_FP-MACCS_deep.txt\n",
      "python Train_Property_ms.py --dataset hiv --feature FP-MACCS --filename deep --project HIV_FP-MACCS_deep  > ./logs/deepnet/hiv_FP-MACCS_deep.txt\n",
      "python Train_Property_ms.py --dataset esol --feature FP-MACCS --filename deep --project ESOL_FP-MACCS_deep  > ./logs/deepnet/esol_FP-MACCS_deep.txt\n",
      "python Train_Property_ms.py --dataset freesolv --feature FP-MACCS --filename deep --project FREESOLV_FP-MACCS_deep  > ./logs/deepnet/freesolv_FP-MACCS_deep.txt\n",
      "python Train_Property_ms.py --dataset lipophilicity --feature FP-MACCS --filename deep --project LIPOPHILICITY_FP-MACCS_deep  > ./logs/deepnet/lipophilicity_FP-MACCS_deep.txt\n",
      "python Train_Property_ms.py --dataset bace --feature FP-Morgan --filename deep --project BACE_FP-Morgan_deep  > ./logs/deepnet/bace_FP-Morgan_deep.txt\n",
      "python Train_Property_ms.py --dataset bbbp --feature FP-Morgan --filename deep --project BBBP_FP-Morgan_deep  > ./logs/deepnet/bbbp_FP-Morgan_deep.txt\n",
      "python Train_Property_ms.py --dataset tox21 --feature FP-Morgan --filename deep --project TOX21_FP-Morgan_deep  > ./logs/deepnet/tox21_FP-Morgan_deep.txt\n",
      "python Train_Property_ms.py --dataset toxcast --feature FP-Morgan --filename deep --project TOXCAST_FP-Morgan_deep  > ./logs/deepnet/toxcast_FP-Morgan_deep.txt\n",
      "python Train_Property_ms.py --dataset sider --feature FP-Morgan --filename deep --project SIDER_FP-Morgan_deep  > ./logs/deepnet/sider_FP-Morgan_deep.txt\n",
      "python Train_Property_ms.py --dataset clintox --feature FP-Morgan --filename deep --project CLINTOX_FP-Morgan_deep  > ./logs/deepnet/clintox_FP-Morgan_deep.txt\n",
      "ls\n",
      "python Train_Property_ms.py --dataset hiv --feature FP-Morgan --filename deep --project HIV_FP-Morgan_deep  > ./logs/deepnet/hiv_FP-Morgan_deep.txt\n",
      "python Train_Property_ms.py --dataset esol --feature FP-Morgan --filename deep --project ESOL_FP-Morgan_deep  > ./logs/deepnet/esol_FP-Morgan_deep.txt\n",
      "python Train_Property_ms.py --dataset freesolv --feature FP-Morgan --filename deep --project FREESOLV_FP-Morgan_deep  > ./logs/deepnet/freesolv_FP-Morgan_deep.txt\n",
      "python Train_Property_ms.py --dataset lipophilicity --feature FP-Morgan --filename deep --project LIPOPHILICITY_FP-Morgan_deep  > ./logs/deepnet/lipophilicity_FP-Morgan_deep.txt\n",
      "python Train_Property_ms.py --dataset bace --feature DESC --filename deep --project BACE_DESC_deep  > ./logs/deepnet/bace_DESC_deep.txt\n",
      "python Train_Property_ms.py --dataset bbbp --feature DESC --filename deep --project BBBP_DESC_deep  > ./logs/deepnet/bbbp_DESC_deep.txt\n",
      "python Train_Property_ms.py --dataset tox21 --feature DESC --filename deep --project TOX21_DESC_deep  > ./logs/deepnet/tox21_DESC_deep.txt\n",
      "python Train_Property_ms.py --dataset toxcast --feature DESC --filename deep --project TOXCAST_DESC_deep  > ./logs/deepnet/toxcast_DESC_deep.txt\n",
      "python Train_Property_ms.py --dataset sider --feature DESC --filename deep --project SIDER_DESC_deep  > ./logs/deepnet/sider_DESC_deep.txt\n",
      "python Train_Property_ms.py --dataset clintox --feature DESC --filename deep --project CLINTOX_DESC_deep  > ./logs/deepnet/clintox_DESC_deep.txt\n",
      "python Train_Property_ms.py --dataset hiv --feature DESC --filename deep --project HIV_DESC_deep  > ./logs/deepnet/hiv_DESC_deep.txt\n",
      "python Train_Property_ms.py --dataset esol --feature DESC --filename deep --project ESOL_DESC_deep  > ./logs/deepnet/esol_DESC_deep.txt\n",
      "python Train_Property_ms.py --dataset freesolv --feature DESC --filename deep --project FREESOLV_DESC_deep  > ./logs/deepnet/freesolv_DESC_deep.txt\n",
      "python Train_Property_ms.py --dataset lipophilicity --feature DESC --filename deep --project LIPOPHILICITY_DESC_deep  > ./logs/deepnet/lipophilicity_DESC_deep.txt\n",
      "python Train_Property_ms.py --dataset bace --feature CNN --filename deep --project BACE_CNN_deep  > ./logs/deepnet/bace_CNN_deep.txt\n",
      "python Train_Property_ms.py --dataset bbbp --feature CNN --filename deep --project BBBP_CNN_deep  > ./logs/deepnet/bbbp_CNN_deep.txt\n",
      "python Train_Property_ms.py --dataset tox21 --feature CNN --filename deep --project TOX21_CNN_deep  > ./logs/deepnet/tox21_CNN_deep.txt\n",
      "python Train_Property_ms.py --dataset toxcast --feature CNN --filename deep --project TOXCAST_CNN_deep  > ./logs/deepnet/toxcast_CNN_deep.txt\n",
      "python Train_Property_ms.py --dataset sider --feature CNN --filename deep --project SIDER_CNN_deep  > ./logs/deepnet/sider_CNN_deep.txt\n",
      "python Train_Property_ms.py --dataset clintox --feature CNN --filename deep --project CLINTOX_CNN_deep  > ./logs/deepnet/clintox_CNN_deep.txt\n",
      "ls\n",
      "python Train_Property_ms.py --dataset hiv --feature CNN --filename deep --project HIV_CNN_deep  > ./logs/deepnet/hiv_CNN_deep.txt\n",
      "python Train_Property_ms.py --dataset esol --feature CNN --filename deep --project ESOL_CNN_deep  > ./logs/deepnet/esol_CNN_deep.txt\n",
      "python Train_Property_ms.py --dataset freesolv --feature CNN --filename deep --project FREESOLV_CNN_deep  > ./logs/deepnet/freesolv_CNN_deep.txt\n",
      "python Train_Property_ms.py --dataset lipophilicity --feature CNN --filename deep --project LIPOPHILICITY_CNN_deep  > ./logs/deepnet/lipophilicity_CNN_deep.txt\n",
      "python Train_Property_ms.py --dataset bace --feature ChemBERTa --filename deep --project BACE_ChemBERTa_deep  > ./logs/deepnet/bace_ChemBERTa_deep.txt\n",
      "python Train_Property_ms.py --dataset bbbp --feature ChemBERTa --filename deep --project BBBP_ChemBERTa_deep  > ./logs/deepnet/bbbp_ChemBERTa_deep.txt\n",
      "python Train_Property_ms.py --dataset tox21 --feature ChemBERTa --filename deep --project TOX21_ChemBERTa_deep  > ./logs/deepnet/tox21_ChemBERTa_deep.txt\n",
      "python Train_Property_ms.py --dataset toxcast --feature ChemBERTa --filename deep --project TOXCAST_ChemBERTa_deep  > ./logs/deepnet/toxcast_ChemBERTa_deep.txt\n",
      "python Train_Property_ms.py --dataset sider --feature ChemBERTa --filename deep --project SIDER_ChemBERTa_deep  > ./logs/deepnet/sider_ChemBERTa_deep.txt\n",
      "python Train_Property_ms.py --dataset clintox --feature ChemBERTa --filename deep --project CLINTOX_ChemBERTa_deep  > ./logs/deepnet/clintox_ChemBERTa_deep.txt\n",
      "python Train_Property_ms.py --dataset hiv --feature ChemBERTa --filename deep --project HIV_ChemBERTa_deep  > ./logs/deepnet/hiv_ChemBERTa_deep.txt\n",
      "python Train_Property_ms.py --dataset esol --feature ChemBERTa --filename deep --project ESOL_ChemBERTa_deep  > ./logs/deepnet/esol_ChemBERTa_deep.txt\n",
      "python Train_Property_ms.py --dataset freesolv --feature ChemBERTa --filename deep --project FREESOLV_ChemBERTa_deep  > ./logs/deepnet/freesolv_ChemBERTa_deep.txt\n",
      "python Train_Property_ms.py --dataset lipophilicity --feature ChemBERTa --filename deep --project LIPOPHILICITY_ChemBERTa_deep  > ./logs/deepnet/lipophilicity_ChemBERTa_deep.txt\n",
      "python Train_Property_ms.py --dataset bace --feature 2D-GNN --filename deep --project BACE_2D-GNN_deep  > ./logs/deepnet/bace_2D-GNN_deep.txt\n",
      "python Train_Property_ms.py --dataset bbbp --feature 2D-GNN --filename deep --project BBBP_2D-GNN_deep  > ./logs/deepnet/bbbp_2D-GNN_deep.txt\n",
      "python Train_Property_ms.py --dataset tox21 --feature 2D-GNN --filename deep --project TOX21_2D-GNN_deep  > ./logs/deepnet/tox21_2D-GNN_deep.txt\n",
      "python Train_Property_ms.py --dataset toxcast --feature 2D-GNN --filename deep --project TOXCAST_2D-GNN_deep  > ./logs/deepnet/toxcast_2D-GNN_deep.txt\n",
      "python Train_Property_ms.py --dataset sider --feature 2D-GNN --filename deep --project SIDER_2D-GNN_deep  > ./logs/deepnet/sider_2D-GNN_deep.txt\n",
      "python Train_Property_ms.py --dataset clintox --feature 2D-GNN --filename deep --project CLINTOX_2D-GNN_deep  > ./logs/deepnet/clintox_2D-GNN_deep.txt\n",
      "ls\n",
      "python Train_Property_ms.py --dataset hiv --feature 2D-GNN --filename deep --project HIV_2D-GNN_deep  > ./logs/deepnet/hiv_2D-GNN_deep.txt\n",
      "python Train_Property_ms.py --dataset esol --feature 2D-GNN --filename deep --project ESOL_2D-GNN_deep  > ./logs/deepnet/esol_2D-GNN_deep.txt\n",
      "python Train_Property_ms.py --dataset freesolv --feature 2D-GNN --filename deep --project FREESOLV_2D-GNN_deep  > ./logs/deepnet/freesolv_2D-GNN_deep.txt\n",
      "python Train_Property_ms.py --dataset lipophilicity --feature 2D-GNN --filename deep --project LIPOPHILICITY_2D-GNN_deep  > ./logs/deepnet/lipophilicity_2D-GNN_deep.txt\n",
      "python Train_Property_ms.py --dataset bace --feature 3D-GNN --filename deep --project BACE_3D-GNN_deep  > ./logs/deepnet/bace_3D-GNN_deep.txt\n",
      "python Train_Property_ms.py --dataset bbbp --feature 3D-GNN --filename deep --project BBBP_3D-GNN_deep  > ./logs/deepnet/bbbp_3D-GNN_deep.txt\n",
      "python Train_Property_ms.py --dataset tox21 --feature 3D-GNN --filename deep --project TOX21_3D-GNN_deep  > ./logs/deepnet/tox21_3D-GNN_deep.txt\n",
      "python Train_Property_ms.py --dataset toxcast --feature 3D-GNN --filename deep --project TOXCAST_3D-GNN_deep  > ./logs/deepnet/toxcast_3D-GNN_deep.txt\n",
      "python Train_Property_ms.py --dataset sider --feature 3D-GNN --filename deep --project SIDER_3D-GNN_deep  > ./logs/deepnet/sider_3D-GNN_deep.txt\n",
      "python Train_Property_ms.py --dataset clintox --feature 3D-GNN --filename deep --project CLINTOX_3D-GNN_deep  > ./logs/deepnet/clintox_3D-GNN_deep.txt\n",
      "python Train_Property_ms.py --dataset hiv --feature 3D-GNN --filename deep --project HIV_3D-GNN_deep  > ./logs/deepnet/hiv_3D-GNN_deep.txt\n",
      "python Train_Property_ms.py --dataset esol --feature 3D-GNN --filename deep --project ESOL_3D-GNN_deep  > ./logs/deepnet/esol_3D-GNN_deep.txt\n",
      "python Train_Property_ms.py --dataset freesolv --feature 3D-GNN --filename deep --project FREESOLV_3D-GNN_deep  > ./logs/deepnet/freesolv_3D-GNN_deep.txt\n",
      "python Train_Property_ms.py --dataset lipophilicity --feature 3D-GNN --filename deep --project LIPOPHILICITY_3D-GNN_deep  > ./logs/deepnet/lipophilicity_3D-GNN_deep.txt\n"
     ]
    }
   ],
   "source": [
    "filename = 'deep'\n",
    "others = ['FP-MACCS', 'FP-Morgan', 'DESC', 'CNN', 'ChemBERTa', '2D-GNN-tuto', '3D-GNN']\n",
    "gemma = ['gemma-desc-mean', 'gemma-desc-max', 'gemma-desc-last', 'gemma-mean', 'gemma-max', 'gemma-last']\n",
    "all = gemma + others\n",
    "\n",
    "count = 0\n",
    "for ft in all:\n",
    "    if ft in gemma:\n",
    "        dts = ['bace', 'bbbp', 'esol', 'lipophilicity']\n",
    "    else:\n",
    "        dts = ['bace', 'bbbp', 'tox21', 'toxcast', 'sider', 'clintox', 'hiv', 'esol', 'freesolv', 'lipophilicity']\n",
    "    \n",
    "    for dt in dts:\n",
    "        cmd = f\"python Train_Property_ms.py --dataset {dt} --feature {ft} --filename {filename} --project {dt.upper()}_{ft}_{filename}  > ./logs/deepnet/{dt}_{ft}_{filename}.txt\"\n",
    "        print(cmd)\n",
    "        count += 1\n",
    "    \n",
    "        if count % 20 == 0:\n",
    "            print('ls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geometric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
